<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WEB AUTOMÃTICO COMPETITIVO - CÃ³digo Python</title>
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css">
    <style>
        .code-block {
            background-color: #1a1a1a;
            color: #f8f8f2;
            font-family: 'Courier New', monospace;
            font-size: 14px;
            line-height: 1.5;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        .python { color: #66d9ef; }
        .string { color: #a6e22e; }
        .comment { color: #75715e; }
        .keyword { color: #f92672; }
        .number { color: #ae81ff; }
        .function { color: #a6e22e; }
        .class { color: #66d9ef; }
        .decorator { color: #fd971f; }
    </style>
</head>
<body class="bg-gray-50 font-sans">
    <!-- Header -->
    <div class="bg-gradient-to-r from-blue-900 to-indigo-900 text-white py-8">
        <div class="container mx-auto px-6">
            <div class="flex items-center justify-between">
                <div>
                    <h1 class="text-4xl font-bold mb-2">
                        <i class="fas fa-robot mr-3"></i>WEB AUTOMÃTICO COMPETITIVO
                    </h1>
                    <p class="text-xl opacity-90">Tier 2.5 - ImplementaciÃ³n Python Completa</p>
                    <p class="text-lg opacity-75 mt-2">Business Intelligence Orchestrator v3.1</p>
                </div>
                <div class="text-right">
                    <div class="bg-green-500 px-4 py-2 rounded-full text-sm font-bold">
                        <i class="fas fa-check-circle mr-2"></i>PRODUCTION READY
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- CaracterÃ­sticas principales -->
    <div class="container mx-auto px-6 py-8">
        <div class="bg-white rounded-lg shadow-lg p-6 mb-8">
            <h2 class="text-2xl font-bold text-gray-800 mb-4">
                <i class="fas fa-star text-yellow-500 mr-2"></i>CaracterÃ­sticas del Sistema
            </h2>
            <div class="grid md:grid-cols-2 lg:grid-cols-4 gap-6">
                <div class="text-center p-4 bg-blue-50 rounded-lg">
                    <i class="fas fa-spider text-3xl text-blue-600 mb-2"></i>
                    <h3 class="font-semibold text-gray-800">Scraping Inteligente</h3>
                    <p class="text-sm text-gray-600">24/7 con rotaciÃ³n automÃ¡tica</p>
                </div>
                <div class="text-center p-4 bg-green-50 rounded-lg">
                    <i class="fas fa-brain text-3xl text-green-600 mb-2"></i>
                    <h3 class="font-semibold text-gray-800">AnÃ¡lisis IA</h3>
                    <p class="text-sm text-gray-600">Sentimientos y comportamiento</p>
                </div>
                <div class="text-center p-4 bg-purple-50 rounded-lg">
                    <i class="fas fa-bell text-3xl text-purple-600 mb-2"></i>
                    <h3 class="font-semibold text-gray-800">Alertas Inteligentes</h3>
                    <p class="text-sm text-gray-600">Notificaciones en tiempo real</p>
                </div>
                <div class="text-center p-4 bg-red-50 rounded-lg">
                    <i class="fas fa-shield-alt text-3xl text-red-600 mb-2"></i>
                    <h3 class="font-semibold text-gray-800">Anti-DetecciÃ³n</h3>
                    <p class="text-sm text-gray-600">Proxies y headers dinÃ¡micos</p>
                </div>
            </div>
        </div>

        <!-- Estructura de archivos -->
        <div class="bg-white rounded-lg shadow-lg p-6 mb-8">
            <h2 class="text-2xl font-bold text-gray-800 mb-4">
                <i class="fas fa-folder-tree text-blue-600 mr-2"></i>Estructura de Archivos
            </h2>
            <div class="code-block">web-automatico-competitivo/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.py
â”‚   â”œâ”€â”€ proxies.json
â”‚   â””â”€â”€ competitors.json
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ competitive_scraper.py
â”‚   â”‚   â”œâ”€â”€ proxy_manager.py
â”‚   â”‚   â””â”€â”€ anti_detection.py
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ sentiment_analyzer.py
â”‚   â”‚   â”œâ”€â”€ price_tracker.py
â”‚   â”‚   â””â”€â”€ change_detector.py
â”‚   â”œâ”€â”€ alerts/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ alert_manager.py
â”‚   â”‚   â””â”€â”€ notification_sender.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ database.py
â”‚       â”œâ”€â”€ cache_manager.py
â”‚       â””â”€â”€ logger.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ start.sh
â”‚   â””â”€â”€ monitor.sh
â””â”€â”€ tests/
    â”œâ”€â”€ test_scraper.py
    â””â”€â”€ test_analysis.py</div>
        </div>

        <!-- Dockerfile -->
        <div class="bg-white rounded-lg shadow-lg p-6 mb-8">
            <h2 class="text-2xl font-bold text-gray-800 mb-4">
                <i class="fab fa-docker text-blue-600 mr-2"></i>Dockerfile
            </h2>
            <div class="code-block"># Multi-stage build para optimizaciÃ³n
FROM python:3.11-slim as builder

# Instalar dependencias del sistema
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    libffi-dev \
    libssl-dev \
    curl \
    wget \
    chromium \
    chromium-driver \
    && rm -rf /var/lib/apt/lists/*

# Crear directorio de trabajo
WORKDIR /app

# Copiar archivos de dependencias
COPY requirements.txt .

# Instalar dependencias Python
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Etapa de producciÃ³n
FROM python:3.11-slim

# Instalar dependencias mÃ­nimas de runtime
RUN apt-get update && apt-get install -y \
    chromium \
    chromium-driver \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Crear usuario no-root para seguridad
RUN useradd -m -u 1000 scraper && \
    mkdir -p /app /data /logs && \
    chown -R scraper:scraper /app /data /logs

WORKDIR /app

# Copiar dependencias desde builder
COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY --from=builder /usr/local/bin /usr/local/bin

# Copiar cÃ³digo fuente
COPY --chown=scraper:scraper . .

# Configurar variables de entorno
ENV PYTHONPATH=/app \
    PYTHONUNBUFFERED=1 \
    CHROME_BIN=/usr/bin/chromium \
    CHROME_PATH=/usr/bin/chromium

# Cambiar a usuario no-root
USER scraper

# Exponer puerto
EXPOSE 8080

# Comando de inicio
CMD ["python", "src/main.py"]</div>
        </div>

        <!-- Requirements.txt -->
        <div class="bg-white rounded-lg shadow-lg p-6 mb-8">
            <h2 class="text-2xl font-bold text-gray-800 mb-4">
                <i class="fas fa-list text-green-600 mr-2"></i>requirements.txt
            </h2>
            <div class="code-block"># Web Scraping Core
requests==2.31.0
selenium==4.15.2
beautifulsoup4==4.12.2
scrapy==2.11.0
playwright==1.40.0

# Anti-Detection & Proxies
fake-useragent==1.4.0
requests-html==0.10.0
undetected-chromedriver==3.5.4
selenium-stealth==1.0.6

# Data Processing & Analysis
pandas==2.1.3
numpy==1.25.2
scikit-learn==1.3.2
textblob==0.17.1
vaderSentiment==3.3.2
transformers==4.35.2

# Database & Caching
asyncpg==0.29.0
redis==5.0.1
sqlalchemy==2.0.23
alembic==1.12.1

# API & Web Framework
fastapi==0.104.1
uvicorn==0.24.0
pydantic==2.5.0
httpx==0.25.2

# Monitoring & Logging
prometheus-client==0.19.0
structlog==23.2.0
sentry-sdk==1.38.0

# Task Queue & Scheduling
celery==5.3.4
apscheduler==3.10.4
kombu==5.3.4

# Utilities
python-dotenv==1.0.0
click==8.1.7
tqdm==4.66.1
pytz==2023.3
pillow==10.1.0

# Testing
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-mock==3.12.0</div>
        </div>

        <!-- CÃ³digo Principal -->
        <div class="bg-white rounded-lg shadow-lg p-6 mb-8">
            <h2 class="text-2xl font-bold text-gray-800 mb-4">
                <i class="fas fa-code text-purple-600 mr-2"></i>src/main.py - AplicaciÃ³n Principal
            </h2>
            <div class="code-block">#!/usr/bin/env python3
"""
WEB AUTOMÃTICO COMPETITIVO Tier 2.5
Business Intelligence Orchestrator v3.1

AplicaciÃ³n principal del sistema de monitoreo competitivo 24/7
"""

import asyncio
import logging
import sys
from pathlib import Path
from typing import Dict, List, Optional

from fastapi import FastAPI, BackgroundTasks, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from prometheus_client import start_http_server, Counter, Histogram, Gauge

# Importar mÃ³dulos locales
sys.path.append(str(Path(__file__).parent))

from scraper.competitive_scraper import CompetitiveScraper
from analysis.sentiment_analyzer import SentimentAnalyzer
from analysis.price_tracker import PriceTracker
from analysis.change_detector import ChangeDetector
from alerts.alert_manager import AlertManager
from utils.database import DatabaseManager
from utils.cache_manager import CacheManager
from utils.logger import setup_logging
from config.settings import Settings

# Configurar logging
logger = setup_logging()

# MÃ©tricas Prometheus
SCRAPING_REQUESTS = Counter('scraping_requests_total', 'Total scraping requests', ['competitor', 'status'])
SCRAPING_DURATION = Histogram('scraping_duration_seconds', 'Scraping duration')
ACTIVE_COMPETITORS = Gauge('active_competitors_count', 'Number of active competitors being monitored')

# ConfiguraciÃ³n
settings = Settings()

class WebAutomaticoCompetitivo:
    """
    Clase principal del WEB AUTOMÃTICO COMPETITIVO Tier 2.5
    
    CaracterÃ­sticas:
    - Scraping inteligente 24/7
    - AnÃ¡lisis de sentimientos en tiempo real
    - DetecciÃ³n automÃ¡tica de cambios
    - Sistema de alertas multichannel
    - Anti-detecciÃ³n avanzada
    """
    
    def __init__(self):
        self.scraper = CompetitiveScraper(settings)
        self.sentiment_analyzer = SentimentAnalyzer()
        self.price_tracker = PriceTracker()
        self.change_detector = ChangeDetector()
        self.alert_manager = AlertManager(settings)
        self.db_manager = DatabaseManager(settings.database_url)
        self.cache_manager = CacheManager(settings.redis_url)
        self.scheduler = AsyncIOScheduler()
        self.is_running = False
        
        logger.info("ğŸš€ WEB AUTOMÃTICO COMPETITIVO Tier 2.5 iniciado")
    
    async def initialize(self):
        """Inicializar todos los componentes del sistema"""
        try:
            # Inicializar base de datos
            await self.db_manager.initialize()
            logger.info("âœ… Base de datos inicializada")
            
            # Inicializar cache
            await self.cache_manager.initialize()
            logger.info("âœ… Sistema de cache inicializado")
            
            # Configurar scheduler para monitoreo automÃ¡tico
            self.scheduler.add_job(
                self.monitor_competitors_cycle,
                'interval',
                minutes=15,  # Monitoreo cada 15 minutos
                id='competitive_monitoring',
                max_instances=1
            )
            
            # Job para anÃ¡lisis profundo cada 2 horas
            self.scheduler.add_job(
                self.deep_analysis_cycle,
                'interval',
                hours=2,
                id='deep_analysis',
                max_instances=1
            )
            
            # Job para limpiar cache cada 6 horas
            self.scheduler.add_job(
                self.cache_cleanup,
                'interval',
                hours=6,
                id='cache_cleanup'
            )
            
            self.scheduler.start()
            logger.info("âœ… Scheduler configurado y iniciado")
            
            self.is_running = True
            
        except Exception as e:
            logger.error(f"âŒ Error inicializando sistema: {e}")
            raise
    
    async def monitor_competitors_cycle(self):
        """Ciclo principal de monitoreo competitivo"""
        if not self.is_running:
            return
            
        logger.info("ğŸ”„ Iniciando ciclo de monitoreo competitivo")
        
        try:
            # Obtener lista de competidores activos
            competitors = await self.db_manager.get_active_competitors()
            ACTIVE_COMPETITORS.set(len(competitors))
            
            tasks = []
            for competitor in competitors:
                task = asyncio.create_task(
                    self.monitor_single_competitor(competitor)
                )
                tasks.append(task)
            
            # Ejecutar monitoreo en paralelo
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Procesar resultados
            successful = sum(1 for r in results if not isinstance(r, Exception))
            failed = len(results) - successful
            
            logger.info(f"âœ… Ciclo completado: {successful} exitosos, {failed} fallidos")
            
            # Enviar resumen si hay fallos
            if failed > 0:
                await self.alert_manager.send_system_alert(
                    f"Monitoreo completado con {failed} fallos de {len(competitors)} competidores"
                )
        
        except Exception as e:
            logger.error(f"âŒ Error en ciclo de monitoreo: {e}")
            await self.alert_manager.send_error_alert(f"Error crÃ­tico en monitoreo: {str(e)}")
    
    @SCRAPING_DURATION.time()
    async def monitor_single_competitor(self, competitor: Dict):
        """Monitorear un competidor especÃ­fico"""
        competitor_name = competitor['name']
        
        try:
            logger.info(f"ğŸ¯ Monitoreando {competitor_name}")
            
            # Scraping de datos
            scraping_data = await self.scraper.scrape_competitor(competitor)
            
            if not scraping_data:
                SCRAPING_REQUESTS.labels(competitor=competitor_name, status='failed').inc()
                return
            
            SCRAPING_REQUESTS.labels(competitor=competitor_name, status='success').inc()
            
            # AnÃ¡lisis de sentimientos en reviews/contenido
            if 'reviews' in scraping_data:
                sentiment_score = await self.sentiment_analyzer.analyze_reviews(
                    scraping_data['reviews']
                )
                scraping_data['sentiment_score'] = sentiment_score
            
            # Tracking de precios
            if 'prices' in scraping_data:
                price_changes = await self.price_tracker.track_prices(
                    competitor_name, scraping_data['prices']
                )
                scraping_data['price_changes'] = price_changes
            
            # DetecciÃ³n de cambios significativos
            changes = await self.change_detector.detect_changes(
                competitor_name, scraping_data
            )
            
            # Almacenar datos en base de datos
            await self.db_manager.store_competitor_data(competitor_name, scraping_data)
            
            # Cache de datos para acceso rÃ¡pido
            await self.cache_manager.cache_competitor_data(
                competitor_name, scraping_data, ttl=900  # 15 minutos
            )
            
            # Enviar alertas si hay cambios significativos
            if changes:
                await self.alert_manager.send_change_alerts(competitor_name, changes)
            
            logger.info(f"âœ… {competitor_name} procesado exitosamente")
        
        except Exception as e:
            logger.error(f"âŒ Error monitoreando {competitor_name}: {e}")
            SCRAPING_REQUESTS.labels(competitor=competitor_name, status='error').inc()
            raise
    
    async def deep_analysis_cycle(self):
        """AnÃ¡lisis profundo cada 2 horas"""
        logger.info("ğŸ§  Iniciando anÃ¡lisis profundo")
        
        try:
            # AnÃ¡lisis de tendencias de precios
            price_trends = await self.price_tracker.analyze_trends()
            
            # AnÃ¡lisis de sentimientos agregado
            sentiment_trends = await self.sentiment_analyzer.analyze_trends()
            
            # AnÃ¡lisis competitivo comparativo
            competitive_analysis = await self.analyze_competitive_landscape()
            
            # Generar insights automÃ¡ticos
            insights = {
                'price_trends': price_trends,
                'sentiment_trends': sentiment_trends,
                'competitive_analysis': competitive_analysis,
                'timestamp': asyncio.get_event_loop().time()
            }
            
            # Almacenar insights
            await self.db_manager.store_insights(insights)
            
            # Enviar reporte ejecutivo si hay insights importantes
            if self.has_important_insights(insights):
                await self.alert_manager.send_executive_report(insights)
            
            logger.info("âœ… AnÃ¡lisis profundo completado")
        
        except Exception as e:
            logger.error(f"âŒ Error en anÃ¡lisis profundo: {e}")
    
    async def analyze_competitive_landscape(self) -> Dict:
        """Analizar panorama competitivo"""
        # Obtener datos recientes de todos los competidores
        competitors_data = await self.db_manager.get_recent_competitors_data(hours=24)
        
        analysis = {
            'market_leaders': [],
            'price_positioning': {},
            'sentiment_ranking': {},
            'activity_levels': {},
            'recommendations': []
        }
        
        # AnÃ¡lisis de lÃ­deres de mercado (por actividad y cambios)
        for competitor, data_list in competitors_data.items():
            activity_level = len(data_list)
            avg_sentiment = sum(d.get('sentiment_score', 0) for d in data_list) / len(data_list)
            
            analysis['activity_levels'][competitor] = activity_level
            analysis['sentiment_ranking'][competitor] = avg_sentiment
        
        # Generar recomendaciones basadas en anÃ¡lisis
        analysis['recommendations'] = self.generate_recommendations(analysis)
        
        return analysis
    
    def has_important_insights(self, insights: Dict) -> bool:
        """Determinar si los insights son lo suficientemente importantes para alertar"""
        # LÃ³gica para determinar insights importantes
        price_changes = insights.get('price_trends', {}).get('significant_changes', [])
        sentiment_changes = insights.get('sentiment_trends', {}).get('major_shifts', [])
        
        return len(price_changes) > 0 or len(sentiment_changes) > 0
    
    def generate_recommendations(self, analysis: Dict) -> List[str]:
        """Generar recomendaciones automÃ¡ticas basadas en anÃ¡lisis"""
        recommendations = []
        
        # Analizar posicionamiento de precios
        price_data = analysis.get('price_positioning', {})
        if price_data:
            # LÃ³gica para recomendaciones de pricing
            pass
        
        # Analizar sentimientos
        sentiment_data = analysis.get('sentiment_ranking', {})
        if sentiment_data:
            # Recomendaciones basadas en sentimientos
            best_sentiment = max(sentiment_data.items(), key=lambda x: x[1])
            recommendations.append(
                f"ğŸ¯ {best_sentiment[0]} lidera en satisfacciÃ³n del cliente con score {best_sentiment[1]:.2f}"
            )
        
        return recommendations
    
    async def cache_cleanup(self):
        """Limpiar cache periÃ³dicamente"""
        logger.info("ğŸ§¹ Iniciando limpieza de cache")
        await self.cache_manager.cleanup_expired()
        logger.info("âœ… Limpieza de cache completada")
    
    async def get_competitor_status(self) -> Dict:
        """Obtener estado actual de todos los competidores"""
        competitors = await self.db_manager.get_active_competitors()
        status = {}
        
        for competitor in competitors:
            latest_data = await self.cache_manager.get_competitor_data(competitor['name'])
            status[competitor['name']] = {
                'active': competitor['active'],
                'last_update': latest_data.get('timestamp') if latest_data else None,
                'status': 'online' if latest_data else 'offline'
            }
        
        return status
    
    async def shutdown(self):
        """Apagar sistema gracefully"""
        logger.info("ğŸ›‘ Iniciando apagado del sistema...")
        
        self.is_running = False
        
        if self.scheduler.running:
            self.scheduler.shutdown()
        
        await self.db_manager.close()
        await self.cache_manager.close()
        
        logger.info("âœ… Sistema apagado correctamente")

# Crear instancia global
web_automatico = WebAutomaticoCompetitivo()

# FastAPI App
app = FastAPI(
    title="WEB AUTOMÃTICO COMPETITIVO API",
    description="Tier 2.5 - Sistema de Monitoreo Competitivo 24/7",
    version="3.1.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
async def startup_event():
    """Inicializar aplicaciÃ³n"""
    await web_automatico.initialize()
    # Iniciar servidor de mÃ©tricas Prometheus
    start_http_server(8000)

@app.on_event("shutdown")
async def shutdown_event():
    """Apagar aplicaciÃ³n"""
    await web_automatico.shutdown()

@app.get("/")
async def root():
    """Endpoint raÃ­z"""
    return {
        "service": "WEB AUTOMÃTICO COMPETITIVO",
        "version": "Tier 2.5",
        "status": "running" if web_automatico.is_running else "stopped"
    }

@app.get("/status")
async def get_status():
    """Obtener estado del sistema"""
    return await web_automatico.get_competitor_status()

@app.post("/manual-scan/{competitor_name}")
async def manual_scan(competitor_name: str, background_tasks: BackgroundTasks):
    """Ejecutar scan manual de un competidor"""
    competitor = await web_automatico.db_manager.get_competitor(competitor_name)
    if not competitor:
        raise HTTPException(status_code=404, detail="Competidor no encontrado")
    
    background_tasks.add_task(web_automatico.monitor_single_competitor, competitor)
    return {"message": f"Scan manual iniciado para {competitor_name}"}

@app.get("/insights")
async def get_insights():
    """Obtener Ãºltimos insights generados"""
    return await web_automatico.db_manager.get_latest_insights()

if __name__ == "__main__":
    # Ejecutar aplicaciÃ³n
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8080,
        reload=False,
        log_level="info"
    )</div>
        </div>

        <!-- Scraper Competitivo -->
        <div class="bg-white rounded-lg shadow-lg p-6 mb-8">
            <h2 class="text-2xl font-bold text-gray-800 mb-4">
                <i class="fas fa-spider text-red-600 mr-2"></i>src/scraper/competitive_scraper.py
            </h2>
            <div class="code-block">"""
Scraper competitivo inteligente con anti-detecciÃ³n avanzada
"""

import asyncio
import random
import time
from typing import Dict, List, Optional, Any
from urllib.parse import urljoin, urlparse
import json

import aiohttp
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import undetected_chromedriver as uc

from .proxy_manager import ProxyManager
from .anti_detection import AntiDetectionManager
from ..utils.logger import get_logger

logger = get_logger(__name__)

class CompetitiveScraper:
    """
    Scraper competitivo avanzado con mÃºltiples estrategias de extracciÃ³n
    
    CaracterÃ­sticas:
    - RotaciÃ³n automÃ¡tica de proxies
    - Headers dinÃ¡micos y user agents
    - DetecciÃ³n de captchas y bypass
    - Scraping de JavaScript con Selenium
    - Rate limiting inteligente
    - Persistencia de sesiones
    """
    
    def __init__(self, settings):
        self.settings = settings
        self.proxy_manager = ProxyManager(settings)
        self.anti_detection = AntiDetectionManager()
        self.ua = UserAgent()
        self.session_pool = {}
        self.driver_pool = {}
        
        # Configuraciones de scraping por tipo de sitio
        self.site_configs = {
            'ecommerce': {
                'selectors': {
                    'price': ['.price', '.cost', '[data-price]', '.amount'],
                    'title': ['h1', '.title', '.product-name', '[data-title]'],
                    'description': ['.description', '.details', '.summary'],
                    'reviews': ['.review', '.comment', '.feedback'],
                    'rating': ['.rating', '.stars', '[data-rating]'],
                    'availability': ['.stock', '.availability', '.in-stock']
                },
                'wait_time': (2, 5),
                'scroll_behavior': True
            },
            'news': {
                'selectors': {
                    'title': ['h1', '.headline', '.title'],
                    'content': ['.content', '.article-body', '.text'],
                    'date': ['.date', '.published', '.timestamp'],
                    'author': ['.author', '.byline', '.writer']
                },
                'wait_time': (1, 3),
                'scroll_behavior': False
            },
            'social': {
                'selectors': {
                    'posts': ['.post', '.tweet', '.update'],
                    'engagement': ['.likes', '.shares', '.comments'],
                    'followers': ['.followers', '.following']
                },
                'wait_time': (3, 7),
                'scroll_behavior': True,
                'requires_login': True
            }
        }
    
    async def scrape_competitor(self, competitor: Dict) -> Optional[Dict]:
        """
        Scraper principal para un competidor
        
        Args:
            competitor: Diccionario con datos del competidor
            
        Returns:
            Datos extraÃ­dos del competidor o None si falla
        """
        competitor_name = competitor['name']
        urls = competitor.get('urls', [])
        site_type = competitor.get('type', 'ecommerce')
        
        logger.info(f"ğŸ¯ Iniciando scraping de {competitor_name} ({len(urls)} URLs)")
        
        all_data = {
            'competitor': competitor_name,
            'timestamp': time.time(),
            'data': {},
            'metadata': {
                'site_type': site_type,
                'urls_processed': 0,
                'errors': []
            }
        }
        
        # Seleccionar estrategia de scraping
        strategy = self.select_scraping_strategy(competitor)
        
        for url_config in urls:
            url = url_config.get('url') if isinstance(url_config, dict) else url_config
            page_type = url_config.get('type', 'main') if isinstance(url_config, dict) else 'main'
            
            try:
                logger.info(f"ğŸ“„ Procesando {url} (tipo: {page_type})")
                
                page_data = await self.scrape_single_page(
                    url, site_type, page_type, strategy
                )
                
                if page_data:
                    all_data['data'][page_type] = page_data
                    all_data['metadata']['urls_processed'] += 1
                    
                    # Esperar entre pÃ¡ginas para evitar detecciÃ³n
                    await self.smart_delay(site_type)
                
            except Exception as e:
                error_msg = f"Error scraping {url}: {str(e)}"
                logger.error(error_msg)
                all_data['metadata']['errors'].append(error_msg)
        
        # Post-procesamiento de datos
        processed_data = await self.post_process_data(all_data, competitor)
        
        success_rate = (all_data['metadata']['urls_processed'] / len(urls)) * 100
        logger.info(f"âœ… Scraping de {competitor_name} completado: {success_rate:.1f}% Ã©xito")
        
        return processed_data if success_rate > 30 else None
    
    def select_scraping_strategy(self, competitor: Dict) -> str:
        """Seleccionar la mejor estrategia de scraping para el competidor"""
        
        # Analizar caracterÃ­sticas del sitio
        urls = competitor.get('urls', [])
        main_url = urls[0] if urls else ''
        
        if isinstance(main_url, dict):
            main_url = main_url.get('url', '')
        
        domain = urlparse(main_url).netloc.lower()
        
        # Estrategias especÃ­ficas por dominio conocido
        if any(platform in domain for platform in ['shopify', 'woocommerce', 'magento']):
            return 'ecommerce_advanced'
        elif any(social in domain for social in ['facebook', 'instagram', 'twitter', 'linkedin']):
            return 'social_media'
        elif any(news in domain for news in ['wordpress', 'drupal', 'news']):
            return 'content_focused'
        else:
            return 'hybrid'
    
    async def scrape_single_page(self, url: str, site_type: str, page_type: str, strategy: str) -> Optional[Dict]:
        """Scraper de pÃ¡gina individual con mÃºltiples fallbacks"""
        
        # Intentar diferentes mÃ©todos de scraping
        methods = ['requests_html', 'selenium', 'playwright']
        
        if strategy == 'social_media':
            methods = ['selenium', 'playwright']  # Social media requiere JS
        
        for method in methods:
            try:
                logger.debug(f"ğŸ”§ Intentando mÃ©todo {method} para {url}")
                
                if method == 'requests_html':
                    data = await self.scrape_with_requests(url, site_type, page_type)
                elif method == 'selenium':
                    data = await self.scrape_with_selenium(url, site_type, page_type)
                elif method == 'playwright':
                    data = await self.scrape_with_playwright(url, site_type, page_type)
                
                if data and self.validate_scraped_data(data, site_type):
                    logger.debug(f"âœ… MÃ©todo {method} exitoso para {url}")
                    return data
                
            except Exception as e:
                logger.warning(f"âš ï¸ MÃ©todo {method} fallÃ³ para {url}: {str(e)}")
                continue
        
        logger.error(f"âŒ Todos los mÃ©todos fallaron para {url}")
        return None
    
    async def scrape_with_requests(self, url: str, site_type: str, page_type: str) -> Dict:
        """Scraping con requests + BeautifulSoup (mÃ¡s rÃ¡pido)"""
        
        # Obtener proxy y headers
        proxy = await self.proxy_manager.get_proxy()
        headers = self.anti_detection.get_random_headers()
        
        async with aiohttp.ClientSession(
            headers=headers,
            timeout=aiohttp.ClientTimeout(total=30),
            connector=aiohttp.TCPConnector(ssl=False)
        ) as session:
            
            proxy_url = f"http://{proxy['ip']}:{proxy['port']}" if proxy else None
            
            async with session.get(url, proxy=proxy_url) as response:
                if response.status != 200:
                    raise Exception(f"HTTP {response.status}")
                
                html = await response.text()
                return self.extract_data_from_html(html, site_type, page_type, url)
    
    async def scrape_with_selenium(self, url: str, site_type: str, page_type: str) -> Dict:
        """Scraping con Selenium (para sitios con JavaScript)"""
        
        driver = await self.get_selenium_driver(site_type)
        
        try:
            # Configurar timeouts
            driver.set_page_load_timeout(30)
            driver.implicitly_wait(10)
            
            # Navegar a la pÃ¡gina
            driver.get(url)
            
            # Esperar a que la pÃ¡gina cargue completamente
            await self.wait_for_page_load(driver, site_type)
            
            # Manejar popups y modales comunes
            await self.handle_popups(driver)
            
            # Scroll si es necesario para sitios tipo ecommerce
            if self.site_configs[site_type].get('scroll_behavior'):
                await self.smart_scroll(driver)
            
            # Extraer datos
            html = driver.page_source
            data = self.extract_data_from_html(html, site_type, page_type, url)
            
            # Capturar screenshot para debugging
            if self.settings.debug_mode:
                screenshot_path = f"/tmp/screenshot_{int(time.time())}.png"
                driver.save_screenshot(screenshot_path)
                data['debug_screenshot'] = screenshot_path
            
            return data
            
        finally:
            await self.return_selenium_driver(driver, site_type)
    
    async def scrape_with_playwright(self, url: str, site_type: str, page_type: str) -> Dict:
        """Scraping con Playwright (alternativa moderna a Selenium)"""
        from playwright.async_api import async_playwright
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-gpu'
                ]
            )
            
            context = await browser.new_context(
                user_agent=self.ua.random,
                viewport={'width': 1920, 'height': 1080}
            )
            
            page = await context.new_page()
            
            try:
                await page.goto(url, wait_until='networkidle', timeout=30000)
                
                # Manejar cookies y popups
                await self.handle_playwright_popups(page)
                
                # Scroll si es necesario
                if self.site_configs[site_type].get('scroll_behavior'):
                    await self.playwright_smart_scroll(page)
                
                html = await page.content()
                return self.extract_data_from_html(html, site_type, page_type, url)
                
            finally:
                await context.close()
                await browser.close()
    
    def extract_data_from_html(self, html: str, site_type: str, page_type: str, url: str) -> Dict:
        """Extraer datos especÃ­ficos del HTML segÃºn el tipo de sitio"""
        
        soup = BeautifulSoup(html, 'html.parser')
        config = self.site_configs.get(site_type, self.site_configs['ecommerce'])
        
        extracted_data = {
            'url': url,
            'page_type': page_type,
            'extraction_time': time.time(),
            'data': {}
        }
        
        # Extraer datos segÃºn selectores configurados
        for data_type, selectors in config['selectors'].items():
            values = []
            
            for selector in selectors:
                elements = soup.select(selector)
                for element in elements[:5]:  # Limitar a 5 elementos por selector
                    text = element.get_text(strip=True)
                    if text and len(text) > 3:  # Filtrar textos muy cortos
                        values.append({
                            'text': text,
                            'selector': selector,
                            'html': str(element)[:500]  # Limitar HTML
                        })
            
            if values:
                extracted_data['data'][data_type] = values
        
        # Extracciones especÃ­ficas por tipo de pÃ¡gina
        if page_type == 'product':
            extracted_data['data'].update(self.extract_product_data(soup))
        elif page_type == 'pricing':
            extracted_data['data'].update(self.extract_pricing_data(soup))
        elif page_type == 'about':
            extracted_data['data'].update(self.extract_company_data(soup))
        
        # Metadata adicional
        extracted_data['metadata'] = {
            'title': soup.title.string if soup.title else '',
            'meta_description': self.get_meta_content(soup, 'description'),
            'canonical_url': self.get_canonical_url(soup),
            'schema_org': self.extract_schema_org(soup),
            'images_count': len(soup.find_all('img')),
            'links_count': len(soup.find_all('a')),
            'word_count': len(soup.get_text().split())
        }
        
        return extracted_data
    
    def extract_product_data(self, soup: BeautifulSoup) -> Dict:
        """Extraer datos especÃ­ficos de productos"""
        product_data = {}
        
        # Buscar datos estructurados JSON-LD
        json_ld_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_ld_scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, dict) and data.get('@type') == 'Product':
                    product_data['structured_data'] = data
                    break
            except:
                continue
        
        # Extraer galerÃ­a de imÃ¡genes
        image_selectors = ['.gallery img', '.product-images img', '[data-gallery] img']
        images = []
        for selector in image_selectors:
            img_elements = soup.select(selector)
            for img in img_elements[:10]:  # MÃ¡ximo 10 imÃ¡genes
                src = img.get('src') or img.get('data-src')
                if src:
                    images.append(src)
        
        product_data['images'] = images
        
        # Extraer variantes/opciones
        variant_selectors = ['.variants select', '.options select', '[data-variants]']
        variants = []
        for selector in variant_selectors:
            elements = soup.select(selector)
            for element in elements:
                options = [opt.get_text(strip=True) for opt in element.find_all('option')]
                if options:
                    variants.append({
                        'type': element.get('name', 'unknown'),
                        'options': options
                    })
        
        product_data['variants'] = variants
        
        return product_data
    
    def extract_pricing_data(self, soup: BeautifulSoup) -> Dict:
        """Extraer datos especÃ­ficos de precios"""
        pricing_data = {}
        
        # Buscar tablas de precios
        price_tables = soup.find_all('table')
        for table in price_tables:
            if any(word in table.get_text().lower() for word in ['price', 'plan', 'cost', 'fee']):
                pricing_data['price_table'] = str(table)
                break
        
        # Extraer planes/paquetes
        plan_selectors = ['.plan', '.package', '.tier', '[data-plan]']
        plans = []
        for selector in plan_selectors:
            elements = soup.select(selector)
            for element in elements[:5]:  # MÃ¡ximo 5 planes
                plan_info = {
                    'name': '',
                    'price': '',
                    'features': [],
                    'html': str(element)[:1000]
                }
                
                # Buscar nombre del plan
                name_elem = element.find(['h1', 'h2', 'h3', '.title'])
                if name_elem:
                    plan_info['name'] = name_elem.get_text(strip=True)
                
                # Buscar precio
                price_elem = element.find(class_=lambda x: x and 'price' in x.lower())
                if price_elem:
                    plan_info['price'] = price_elem.get_text(strip=True)
                
                # Buscar caracterÃ­sticas
                feature_lists = element.find_all(['ul', 'ol'])
                for ul in feature_lists:
                    features = [li.get_text(strip=True) for li in ul.find_all('li')]
                    plan_info['features'].extend(features)
                
                plans.append(plan_info)
        
        pricing_data['plans'] = plans
        
        return pricing_data
    
    def extract_company_data(self, soup: BeautifulSoup) -> Dict:
        """Extraer datos de la empresa"""
        company_data = {}
        
        # Buscar informaciÃ³n de contacto
        contact_selectors = ['.contact', '.address', '[data-contact]']
        for selector in contact_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text(strip=True)
                if any(word in text.lower() for word in ['email', 'phone', 'address', 'contact']):
                    company_data['contact_info'] = text
                    break
        
        # Buscar equipo/about
        team_selectors = ['.team', '.about', '.staff', '[data-team]']
        team_info = []
        for selector in team_selectors:
            elements = soup.select(selector)
            for element in elements[:3]:  # MÃ¡ximo 3 secciones
                team_info.append(element.get_text(strip=True)[:500])
        
        company_data['team_info'] = team_info
        
        return company_data
    
    def get_meta_content(self, soup: BeautifulSoup, name: str) -> str:
        """Obtener contenido de meta tag"""
        meta = soup.find('meta', attrs={'name': name}) or soup.find('meta', attrs={'property': f'og:{name}'})
        return meta.get('content', '') if meta else ''
    
    def get_canonical_url(self, soup: BeautifulSoup) -> str:
        """Obtener URL canÃ³nica"""
        canonical = soup.find('link', rel='canonical')
        return canonical.get('href', '') if canonical else ''
    
    def extract_schema_org(self, soup: BeautifulSoup) -> Dict:
        """Extraer datos Schema.org"""
        schema_data = {}
        
        # Buscar microdata
        items = soup.find_all(attrs={'itemtype': True})
        for item in items:
            item_type = item.get('itemtype', '')
            if 'schema.org' in item_type:
                schema_data[item_type] = {
                    'properties': {}
                }
                
                props = item.find_all(attrs={'itemprop': True})
                for prop in props:
                    prop_name = prop.get('itemprop')
                    prop_value = prop.get('content') or prop.get_text(strip=True)
                    schema_data[item_type]['properties'][prop_name] = prop_value
        
        return schema_data
    
    async def get_selenium_driver(self, site_type: str) -> webdriver.Chrome:
        """Obtener driver de Selenium del pool o crear uno nuevo"""
        
        if site_type in self.driver_pool and len(self.driver_pool[site_type]) > 0:
            return self.driver_pool[site_type].pop()
        
        # Crear nuevo driver
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument(f'--user-agent={self.ua.random}')
        
        # Configurar proxy si estÃ¡ disponible
        proxy = await self.proxy_manager.get_proxy()
        if proxy:
            options.add_argument(f'--proxy-server=http://{proxy["ip"]}:{proxy["port"]}')
        
        # Configuraciones anti-detecciÃ³n
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        driver = uc.Chrome(options=options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        return driver
    
    async def return_selenium_driver(self, driver: webdriver.Chrome, site_type: str):
        """Devolver driver al pool o cerrarlo si el pool estÃ¡ lleno"""
        
        if site_type not in self.driver_pool:
            self.driver_pool[site_type] = []
        
        if len(self.driver_pool[site_type]) < 3:  # MÃ¡ximo 3 drivers por tipo
            # Limpiar driver antes de devolverlo al pool
            driver.delete_all_cookies()
            driver.execute_script("window.localStorage.clear();")
            driver.execute_script("window.sessionStorage.clear();")
            
            self.driver_pool[site_type].append(driver)
        else:
            driver.quit()
    
    async def wait_for_page_load(self, driver: webdriver.Chrome, site_type: str):
        """Esperar a que la pÃ¡gina cargue completamente"""
        
        config = self.site_configs.get(site_type, self.site_configs['ecommerce'])
        wait_time = random.uniform(*config['wait_time'])
        
        # Esperar por estado de carga
        WebDriverWait(driver, 30).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # Esperar tiempo adicional para JavaScript
        await asyncio.sleep(wait_time)
        
        # Esperar por elementos especÃ­ficos segÃºn el tipo de sitio
        if site_type == 'ecommerce':
            try:
                WebDriverWait(driver, 10).until(
                    EC.any_of(
                        EC.presence_of_element_located((By.CSS_SELECTOR, '.price')),
                        EC.presence_of_element_located((By.CSS_SELECTOR, '.product')),
                        EC.presence_of_element_located((By.CSS_SELECTOR, '[data-price]'))
                    )
                )
            except TimeoutException:
                pass  # No es crÃ­tico si no encuentra estos elementos
    
    async def handle_popups(self, driver: webdriver.Chrome):
        """Manejar popups comunes (cookies, newsletters, etc.)"""
        
        popup_selectors = [
            # Cookies
            '[data-dismiss="modal"]',
            '.cookie-accept',
            '.accept-cookies',
            '#cookie-consent button',
            
            # Newsletters
            '.newsletter-popup .close',
            '.modal .close',
            '.popup-close',
            
            # General close buttons
            '[aria-label="Close"]',
            '.btn-close',
            '.close-button'
        ]
        
        for selector in popup_selectors:
            try:
                element = WebDriverWait(driver, 2).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, selector))
                )
                element.click()
                await asyncio.sleep(0.5)
            except TimeoutException:
                continue
    
    async def smart_scroll(self, driver: webdriver.Chrome):
        """Scroll inteligente para cargar contenido dinÃ¡mico"""
        
        # Obtener altura inicial
        last_height = driver.execute_script("return document.body.scrollHeight")
        
        scroll_attempts = 0
        max_scrolls = 5
        
        while scroll_attempts < max_scrolls:
            # Scroll hacia abajo
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            # Esperar a que cargue contenido nuevo
            await asyncio.sleep(2)
            
            # Calcular nueva altura
            new_height = driver.execute_script("return document.body.scrollHeight")
            
            if new_height == last_height:
                break  # No hay mÃ¡s contenido para cargar
            
            last_height = new_height
            scroll_attempts += 1
        
        # Scroll de vuelta al top
        driver.execute_script("window.scrollTo(0, 0);")
        await asyncio.sleep(1)
    
    async def handle_playwright_popups(self, page):
        """Manejar popups en Playwright"""
        
        popup_selectors = [
            '.cookie-accept',
            '.accept-cookies',
            '[data-dismiss="modal"]',
            '.modal .close',
            '.popup-close'
        ]
        
        for selector in popup_selectors:
            try:
                await page.click(selector, timeout=2000)
                await page.wait_for_timeout(500)
            except:
                continue
    
    async def playwright_smart_scroll(self, page):
        """Scroll inteligente en Playwright"""
        
        await page.evaluate("""
            const scrollStep = () => {
                window.scrollBy(0, window.innerHeight);
            };
            
            const scrollToBottom = () => {
                return new Promise((resolve) => {
                    let totalHeight = 0;
                    const distance = 100;
                    const timer = setInterval(() => {
                        const scrollHeight = document.body.scrollHeight;
                        window.scrollBy(0, distance);
                        totalHeight += distance;
                        
                        if(totalHeight >= scrollHeight){
                            clearInterval(timer);
                            resolve();
                        }
                    }, 100);
                });
            };
            
            await scrollToBottom();
        """)
        
        await page.evaluate("window.scrollTo(0, 0)")
    
    def validate_scraped_data(self, data: Dict, site_type: str) -> bool:
        """Validar que los datos extraÃ­dos sean Ãºtiles"""
        
        if not data or not data.get('data'):
            return False
        
        extracted_data = data['data']
        
        # Validaciones bÃ¡sicas por tipo de sitio
        if site_type == 'ecommerce':
            # Debe tener al menos precio o tÃ­tulo
            return bool(extracted_data.get('price') or extracted_data.get('title'))
        
        elif site_type == 'news':
            # Debe tener tÃ­tulo y contenido
            return bool(extracted_data.get('title') and extracted_data.get('content'))
        
        elif site_type == 'social':
            # Debe tener posts o algÃºn tipo de contenido social
            return bool(extracted_data.get('posts') or extracted_data.get('engagement'))
        
        # ValidaciÃ³n genÃ©rica: debe tener al menos 2 tipos de datos
        return len(extracted_data) >= 2
    
    async def smart_delay(self, site_type: str):
        """Delay inteligente entre requests"""
        
        config = self.site_configs.get(site_type, self.site_configs['ecommerce'])
        min_wait, max_wait = config['wait_time']
        
        # AÃ±adir variabilidad extra
        delay = random.uniform(min_wait, max_wait) + random.uniform(0, 2)
        
        logger.debug(f"â³ Esperando {delay:.2f} segundos...")
        await asyncio.sleep(delay)
    
    async def post_process_data(self, raw_data: Dict, competitor: Dict) -> Dict:
        """Post-procesar datos extraÃ­dos"""
        
        processed_data = {
            'competitor': raw_data['competitor'],
            'timestamp': raw_data['timestamp'],
            'summary': {
                'total_pages': len(raw_data['data']),
                'success_rate': raw_data['metadata']['urls_processed'] / len(competitor.get('urls', [])),
                'errors': len(raw_data['metadata']['errors'])
            },
            'data': {}
        }
        
        # Procesar cada pÃ¡gina
        for page_type, page_data in raw_data['data'].items():
            processed_data['data'][page_type] = {
                'url': page_data['url'],
                'extraction_time': page_data['extraction_time'],
                'processed_data': self.clean_and_structure_data(page_data['data']),
                'metadata': page_data.get('metadata', {})
            }
        
        # Generar insights automÃ¡ticos
        processed_data['insights'] = self.generate_auto_insights(processed_data)
        
        return processed_data
    
    def clean_and_structure_data(self, raw_extracted_data: Dict) -> Dict:
        """Limpiar y estructurar datos extraÃ­dos"""
        
        cleaned_data = {}
        
        for data_type, values in raw_extracted_data.items():
            if not values:
                continue
            
            if data_type == 'price':
                # Limpiar y normalizar precios
                cleaned_data['prices'] = self.normalize_prices(values)
            
            elif data_type in ['title', 'description']:
                # Tomar el valor mÃ¡s relevante
                cleaned_data[data_type] = self.select_best_text(values)
            
            elif data_type == 'reviews':
                # Procesar reviews
                cleaned_data['reviews'] = self.process_reviews(values)
            
            elif data_type == 'rating':
                # Normalizar ratings
                cleaned_data['rating'] = self.normalize_rating(values)
            
            else:
                # Datos genÃ©ricos
                cleaned_data[data_type] = [v['text'] for v in values if v.get('text')]
        
        return cleaned_data
    
    def normalize_prices(self, price_values: List[Dict]) -> List[Dict]:
        """Normalizar precios extraÃ­dos"""
        import re
        
        normalized_prices = []
        
        for price_data in price_values:
            text = price_data['text']
            
            # Extraer nÃºmeros y sÃ­mbolos de moneda
            price_match = re.search(r'[\$â‚¬Â£Â¥â‚¹]\s*([0-9,]+\.?[0-9]*)', text)
            if not price_match:
                price_match = re.search(r'([0-9,]+\.?[0-9]*)\s*[\$â‚¬Â£Â¥â‚¹]', text)
            
            if price_match:
                amount_str = price_match.group(1).replace(',', '')
                try:
                    amount = float(amount_str)
                    currency = re.search(r'[\$â‚¬Â£Â¥â‚¹]', text)
                    
                    normalized_prices.append({
                        'amount': amount,
                        'currency': currency.group() if currency else '$',
                        'original_text': text,
                        'selector': price_data['selector']
                    })
                except ValueError:
                    continue
        
        return normalized_prices
    
    def select_best_text(self, text_values: List[Dict]) -> str:
        """Seleccionar el mejor texto de una lista"""
        
        if not text_values:
            return ''
        
        # Ordenar por longitud (asumir que texto mÃ¡s largo es mÃ¡s descriptivo)
        sorted_texts = sorted(text_values, key=lambda x: len(x['text']), reverse=True)
        
        # Filtrar textos muy cortos o muy largos
        filtered_texts = [
            t for t in sorted_texts 
            if 10 <= len(t['text']) <= 500
        ]
        
        if filtered_texts:
            return filtered_texts[0]['text']
        elif sorted_texts:
            return sorted_texts[0]['text']
        else:
            return ''
    
    def process_reviews(self, review_values: List[Dict]) -> List[Dict]:
        """Procesar reviews extraÃ­das"""
        
        processed_reviews = []
        
        for review_data in review_values:
            text = review_data['text']
            
            # Filtrar reviews muy cortas o muy largas
            if not (20 <= len(text) <= 1000):
                continue
            
            processed_reviews.append({
                'text': text,
                'word_count': len(text.split()),
                'sentiment_preview': 'positive' if any(word in text.lower() for word in ['good', 'great', 'excellent', 'amazing']) else 'neutral',
                'selector': review_data['selector']
            })
        
        return processed_reviews[:10]  # Limitar a 10 reviews
    
    def normalize_rating(self, rating_values: List[Dict]) -> Optional[Dict]:
        """Normalizar ratings"""
        import re
        
        for rating_data in rating_values:
            text = rating_data['text']
            
            # Buscar patrones de rating
            rating_match = re.search(r'([0-5]\.?[0-9]*)\s*(?:out of|/|of)\s*([0-5])', text)
            if not rating_match:
                rating_match = re.search(r'([0-5]\.?[0-9]*)\s*stars?', text)
            
            if rating_match:
                try:
                    score = float(rating_match.group(1))
                    max_score = float(rating_match.group(2)) if len(rating_match.groups()) > 1 else 5
                    
                    # Normalizar a escala de 5
                    normalized_score = (score / max_score) * 5
                    
                    return {
                        'score': normalized_score,
                        'original_score': score,
                        'max_score': max_score,
                        'original_text': text
                    }
                except ValueError:
                    continue
        
        return None
    
    def generate_auto_insights(self, processed_data: Dict) -> Dict:
        """Generar insights automÃ¡ticos de los datos"""
        
        insights = {
            'data_quality': 'high' if processed_data['summary']['success_rate'] > 0.8 else 'medium' if processed_data['summary']['success_rate'] > 0.5 else 'low',
            'content_richness': {},
            'competitive_signals': []
        }
        
        # Analizar riqueza de contenido
        total_data_points = 0
        for page_type, page_info in processed_data['data'].items():
            data_points = len(page_info['processed_data'])
            total_data_points += data_points
            insights['content_richness'][page_type] = data_points
        
        insights['total_data_points'] = total_data_points
        
        # Detectar seÃ±ales competitivas
        for page_type, page_info in processed_data['data'].items():
            processed = page_info['processed_data']
            
            if 'prices' in processed and processed['prices']:
                insights['competitive_signals'].append(f"Price data available on {page_type} page")
            
            if 'reviews' in processed and processed['reviews']:
                insights['competitive_signals'].append(f"Customer reviews available on {page_type} page")
            
            if 'rating' in processed and processed['rating']:
                insights['competitive_signals'].append(f"Rating system detected on {page_type} page")
        
        return insights</div>
        </div>

        <!-- Configuraciones y Scripts -->
        <div class="bg-white rounded-lg shadow-lg p-6 mb-8">
            <h2 class="text-2xl font-bold text-gray-800 mb-4">
                <i class="fas fa-cogs text-green-600 mr-2"></i>Configuraciones y Scripts de Deployment
            </h2>
            
            <!-- Config Settings -->
            <div class="mb-6">
                <h3 class="text-xl font-semibold text-gray-700 mb-3">config/settings.py</h3>
                <div class="code-block">"""
ConfiguraciÃ³n centralizada del WEB AUTOMÃTICO COMPETITIVO
"""

import os
from typing import List, Dict
from pydantic import BaseSettings, validator

class Settings(BaseSettings):
    """ConfiguraciÃ³n del sistema"""
    
    # Base de datos
    database_url: str = "postgresql://user:password@postgres:5432/bi_orchestrator"
    
    # Redis
    redis_url: str = "redis://redis:6379/0"
    
    # API Keys
    openai_api_key: str = ""
    anthropic_api_key: str = ""
    
    # Configuraciones de scraping
    max_concurrent_scrapers: int = 5
    request_timeout: int = 30
    retry_attempts: int = 3
    proxy_rotation_enabled: bool = True
    
    # Anti-detecciÃ³n
    user_agent_rotation: bool = True
    random_delays: bool = True
    min_delay: float = 1.0
    max_delay: float = 5.0
    
    # Alertas
    slack_webhook_url: str = ""
    email_notifications: bool = True
    smtp_host: str = "smtp.gmail.com"
    smtp_port: int = 587
    smtp_username: str = ""
    smtp_password: str = ""
    
    # Monitoreo
    prometheus_enabled: bool = True
    metrics_port: int = 8000
    
    # Debug
    debug_mode: bool = False
    log_level: str = "INFO"
    
    # ConfiguraciÃ³n de industrias
    industry_configs: Dict = {
        "automotive": {
            "competitors": [
                {
                    "name": "Toyota",
                    "urls": [
                        {"url": "https://www.toyota.com/pricing", "type": "pricing"},
                        {"url": "https://www.toyota.com/models", "type": "products"}
                    ],
                    "type": "ecommerce",
                    "active": True
                }
            ],
            "key_metrics": ["pricing", "inventory", "promotions", "reviews"],
            "scraping_frequency": 15  # minutos
        },
        "hospitality": {
            "competitors": [
                {
                    "name": "Marriott",
                    "urls": [
                        {"url": "https://www.marriott.com/rates", "type": "pricing"},
                        {"url": "https://www.marriott.com/reviews", "type": "reviews"}
                    ],
                    "type": "hospitality",
                    "active": True
                }
            ],
            "key_metrics": ["room_rates", "availability", "reviews", "amenities"],
            "scraping_frequency": 30
        }
    }
    
    class Config:
        env_file = ".env"
        case_sensitive = False
    
    @validator('max_concurrent_scrapers')
    def validate_scrapers(cls, v):
        if v < 1 or v > 20:
            raise ValueError('max_concurrent_scrapers must be between 1 and 20')
        return v
    
    @validator('log_level')
    def validate_log_level(cls, v):
        valid_levels = ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']
        if v.upper() not in valid_levels:
            raise ValueError(f'log_level must be one of {valid_levels}')
        return v.upper()

# Instancia global
settings = Settings()</div>
            </div>

            <!-- Scripts de inicio -->
            <div class="mb-6">
                <h3 class="text-xl font-semibold text-gray-700 mb-3">scripts/start.sh</h3>
                <div class="code-block">#!/bin/bash

# Script de inicio del WEB AUTOMÃTICO COMPETITIVO
# Business Intelligence Orchestrator v3.1

set -e

echo "ğŸš€ Iniciando WEB AUTOMÃTICO COMPETITIVO Tier 2.5"
echo "================================================"

# Colores para output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# FunciÃ³n para logging
log() {
    echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')]${NC} $1"
}

error() {
    echo -e "${RED}[ERROR]${NC} $1" >&2
}

warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

# Verificar Docker
if ! command -v docker &> /dev/null; then
    error "Docker no estÃ¡ instalado"
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    error "Docker Compose no estÃ¡ instalado"
    exit 1
fi

# Verificar archivos de configuraciÃ³n
if [ ! -f ".env" ]; then
    warning "Archivo .env no encontrado, creando desde template..."
    cp .env.template .env
fi

if [ ! -f "config/competitors.json" ]; then
    warning "Archivo competitors.json no encontrado, creando configuraciÃ³n por defecto..."
    mkdir -p config
    cat > config/competitors.json << 'EOF'
{
    "automotive": [
        {
            "name": "Toyota",
            "urls": [
                {"url": "https://www.toyota.com/pricing", "type": "pricing"},
                {"url": "https://www.toyota.com/models", "type": "products"}
            ],
            "type": "ecommerce",
            "active": true
        }
    ],
    "hospitality": [
        {
            "name": "Marriott", 
            "urls": [
                {"url": "https://www.marriott.com/rates", "type": "pricing"}
            ],
            "type": "hospitality",
            "active": true
        }
    ]
}
EOF
fi

# Crear directorios necesarios
log "Creando directorios..."
mkdir -p logs data/postgres data/redis screenshots temp

# Verificar y crear network de Docker
NETWORK_NAME="bi-orchestrator-network"
if ! docker network ls | grep -q $NETWORK_NAME; then
    log "Creando network de Docker: $NETWORK_NAME"
    docker network create $NETWORK_NAME
fi

# Construir imÃ¡genes
log "Construyendo imÃ¡genes Docker..."
docker-compose build --parallel

# Iniciar servicios de base de datos primero
log "Iniciando servicios de base de datos..."
docker-compose up -d postgres redis

# Esperar a que los servicios estÃ©n listos
log "Esperando a que PostgreSQL estÃ© listo..."
timeout 60s bash -c 'until docker-compose exec -T postgres pg_isready -U ${POSTGRES_USER:-postgres}; do sleep 2; done'

log "Esperando a que Redis estÃ© listo..."
timeout 30s bash -c 'until docker-compose exec -T redis redis-cli ping | grep -q PONG; do sleep 2; done'

# Ejecutar migraciones de base de datos
log "Ejecutando migraciones de base de datos..."
docker-compose exec -T postgres psql -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-bi_orchestrator} -f /docker-entrypoint-initdb.d/schema.sql
docker-compose exec -T postgres psql -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-bi_orchestrator} -f /docker-entrypoint-initdb.d/functions.sql

# Iniciar todos los servicios
log "Iniciando todos los servicios..."
docker-compose up -d

# Verificar estado de los servicios
log "Verificando estado de los servicios..."
sleep 10

# Lista de servicios esperados
SERVICES=("postgres" "redis" "web-monitor" "n8n" "grafana")

for service in "${SERVICES[@]}"; do
    if docker-compose ps $service | grep -q "Up"; then
        log "âœ… $service estÃ¡ ejecutÃ¡ndose"
    else
        error "âŒ $service no estÃ¡ ejecutÃ¡ndose correctamente"
        docker-compose logs $service
    fi
done

# Verificar conectividad de APIs
log "Verificando APIs..."

# API del WEB AUTOMÃTICO COMPETITIVO
if curl -f -s "http://localhost:8080/" > /dev/null; then
    log "âœ… WEB AUTOMÃTICO COMPETITIVO API disponible en puerto 8080"
else
    warning "âš ï¸ WEB AUTOMÃTICO COMPETITIVO API no responde en puerto 8080"
fi

# N8N
if curl -f -s "http://localhost:5678/" > /dev/null; then
    log "âœ… N8N disponible en puerto 5678"
else
    warning "âš ï¸ N8N no responde en puerto 5678"
fi

# Grafana
if curl -f -s "http://localhost:3000/" > /dev/null; then
    log "âœ… Grafana disponible en puerto 3000"
else
    warning "âš ï¸ Grafana no responde en puerto 3000"
fi

# Mostrar informaciÃ³n de acceso
echo ""
echo -e "${BLUE}================================================${NC}"
echo -e "${BLUE}ğŸ‰ WEB AUTOMÃTICO COMPETITIVO iniciado exitosamente${NC}"
echo -e "${BLUE}================================================${NC}"
echo ""
echo -e "${GREEN}Servicios disponibles:${NC}"
echo -e "ğŸ¤– WEB AUTOMÃTICO COMPETITIVO API: ${YELLOW}http://localhost:8080${NC}"
echo -e "ğŸ”„ N8N Workflows: ${YELLOW}http://localhost:5678${NC}"
echo -e "ğŸ“Š Grafana Dashboard: ${YELLOW}http://localhost:3000${NC}"
echo -e "ğŸ“ˆ Prometheus Metrics: ${YELLOW}http://localhost:9090${NC}"
echo ""
echo -e "${GREEN}Credenciales por defecto:${NC}"
echo -e "N8N: Sin autenticaciÃ³n (primera ejecuciÃ³n)"
echo -e "Grafana: admin/admin"
echo ""
echo -e "${GREEN}Comandos Ãºtiles:${NC}"
echo -e "Ver logs: ${YELLOW}docker-compose logs -f web-monitor${NC}"
echo -e "Monitorear estado: ${YELLOW}./scripts/status.sh${NC}"
echo -e "Detener sistema: ${YELLOW}./scripts/stop.sh${NC}"
echo ""

# Configurar monitoreo automÃ¡tico
log "Configurando monitoreo automÃ¡tico..."
if [ -f "./scripts/monitor.sh" ]; then
    chmod +x ./scripts/monitor.sh
    
    # Crear cron job para monitoreo (opcional)
    CRON_JOB="*/5 * * * * /bin/bash $(pwd)/scripts/monitor.sh >> $(pwd)/logs/monitor.log 2>&1"
    
    if ! crontab -l 2>/dev/null | grep -q "scripts/monitor.sh"; then
        warning "Para habilitar monitoreo automÃ¡tico cada 5 minutos, ejecuta:"
        echo -e "${YELLOW}(crontab -l 2>/dev/null; echo \"$CRON_JOB\") | crontab -${NC}"
    fi
fi

log "ğŸ¯ Sistema completamente operativo y monitoreando competidores 24/7"

# Ejecutar test inicial
if [ "$1" == "--test" ]; then
    log "Ejecutando test inicial..."
    sleep 5
    curl -X POST "http://localhost:8080/manual-scan/test-competitor" || warning "Test inicial fallÃ³"
fi

exit 0</div>
            </div>

            <!-- Script de monitoreo -->
            <div class="mb-6">
                <h3 class="text-xl font-semibold text-gray-700 mb-3">scripts/monitor.sh</h3>
                <div class="code-block">#!/bin/bash

# Script de monitoreo del WEB AUTOMÃTICO COMPETITIVO
# Verifica el estado y rendimiento del sistema

# ConfiguraciÃ³n
API_BASE="http://localhost:8080"
ALERT_THRESHOLD_CPU=80
ALERT_THRESHOLD_MEMORY=85
LOG_FILE="/logs/monitor.log"

# FunciÃ³n para logging con timestamp
log_with_timestamp() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1" | tee -a $LOG_FILE
}

# FunciÃ³n para enviar alertas
send_alert() {
    local message="$1"
    local severity="$2"
    
    log_with_timestamp "ALERT [$severity]: $message"
    
    # Enviar a Slack si estÃ¡ configurado
    if [ -n "$SLACK_WEBHOOK_URL" ]; then
        curl -X POST -H 'Content-type: application/json' \
            --data "{\"text\":\"ğŸš¨ WEB AUTOMÃTICO COMPETITIVO - $severity: $message\"}" \
            "$SLACK_WEBHOOK_URL" &>/dev/null
    fi
    
    # Logs adicionales para severity alta
    if [ "$severity" == "CRITICAL" ]; then
        docker-compose logs --tail=50 web-monitor >> $LOG_FILE
    fi
}

# Verificar si los servicios estÃ¡n ejecutÃ¡ndose
check_services() {
    local failed_services=()
    
    # Lista de servicios crÃ­ticos
    services=("postgres" "redis" "web-monitor")
    
    for service in "${services[@]}"; do
        if ! docker-compose ps $service | grep -q "Up"; then
            failed_services+=($service)
        fi
    done
    
    if [ ${#failed_services[@]} -gt 0 ]; then
        send_alert "Servicios no disponibles: ${failed_services[*]}" "CRITICAL"
        return 1
    fi
    
    return 0
}

# Verificar conectividad de API
check_api_health() {
    local status_code=$(curl -s -o /dev/null -w "%{http_code}" "$API_BASE/")
    
    if [ "$status_code" != "200" ]; then
        send_alert "API no responde correctamente (HTTP $status_code)" "HIGH"
        return 1
    fi
    
    # Verificar endpoint de status
    local status_response=$(curl -s "$API_BASE/status" | jq -r '.status' 2>/dev/null)
    
    if [ "$status_response" != "running" ]; then
        send_alert "Sistema reporta estado: $status_response" "MEDIUM"
        return 1
    fi
    
    return 0
}

# Verificar uso de recursos
check_resource_usage() {
    # CPU usage
    local cpu_usage=$(docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}" | grep web-monitor | awk '{print $2}' | sed 's/%//')
    
    if [ -n "$cpu_usage" ] && (( $(echo "$cpu_usage > $ALERT_THRESHOLD_CPU" | bc -l) )); then
        send_alert "Alto uso de CPU: ${cpu_usage}%" "MEDIUM"
    fi
    
    # Memory usage
    local memory_usage=$(docker stats --no-stream --format "table {{.Container}}\t{{.MemPerc}}" | grep web-monitor | awk '{print $2}' | sed 's/%//')
    
    if [ -n "$memory_usage" ] && (( $(echo "$memory_usage > $ALERT_THRESHOLD_MEMORY" | bc -l) )); then
        send_alert "Alto uso de memoria: ${memory_usage}%" "MEDIUM"
    fi
}

# Verificar logs de errores recientes
check_error_logs() {
    local error_count=$(docker-compose logs --tail=100 web-monitor | grep -i "error\|exception\|failed" | wc -l)
    
    if [ "$error_count" -gt 10 ]; then
        send_alert "Alta cantidad de errores en logs: $error_count en Ãºltimas 100 lÃ­neas" "HIGH"
    fi
}

# Verificar base de datos
check_database() {
    local db_status=$(docker-compose exec -T postgres pg_isready -U postgres 2>/dev/null | grep -o "accepting connections")
    
    if [ "$db_status" != "accepting connections" ]; then
        send_alert "Base de datos PostgreSQL no disponible" "CRITICAL"
        return 1
    fi
    
    # Verificar conectividad desde la aplicaciÃ³n
    local db_test=$(curl -s "$API_BASE/status" | jq -r '.database_status' 2>/dev/null)
    
    if [ "$db_test" == "error" ]; then
        send_alert "AplicaciÃ³n no puede conectar a la base de datos" "CRITICAL"
        return 1
    fi
    
    return 0
}

# Verificar Redis
check_redis() {
    local redis_status=$(docker-compose exec -T redis redis-cli ping 2>/dev/null)
    
    if [ "$redis_status" != "PONG" ]; then
        send_alert "Redis no disponible" "HIGH"
        return 1
    fi
    
    return 0
}

# Verificar espacio en disco
check_disk_space() {
    local disk_usage=$(df -h / | awk 'NR==2 {print $5}' | sed 's/%//')
    
    if [ "$disk_usage" -gt 90 ]; then
        send_alert "Espacio en disco crÃ­tico: ${disk_usage}% usado" "CRITICAL"
    elif [ "$disk_usage" -gt 80 ]; then
        send_alert "Espacio en disco alto: ${disk_usage}% usado" "MEDIUM"
    fi
}

# Verificar trabajos de scraping recientes
check_scraping_activity() {
    # Ver actividad de scraping en los Ãºltimos 30 minutos
    local recent_activity=$(curl -s "$API_BASE/insights" | jq -r '.last_scraping' 2>/dev/null)
    local current_time=$(date +%s)
    
    if [ -n "$recent_activity" ]; then
        local time_diff=$((current_time - recent_activity))
        
        # Si no hay actividad en mÃ¡s de 1 hora (3600 segundos)
        if [ "$time_diff" -gt 3600 ]; then
            send_alert "No hay actividad de scraping en las Ãºltimas $(($time_diff / 60)) minutos" "MEDIUM"
        fi
    fi
}

# Ejecutar todas las verificaciones
run_health_checks() {
    log_with_timestamp "Iniciando verificaciones de salud del sistema..."
    
    local checks_passed=0
    local total_checks=7
    
    # Ejecutar verificaciones
    check_services && ((checks_passed++))
    check_api_health && ((checks_passed++))
    check_database && ((checks_passed++))
    check_redis && ((checks_passed++))
    check_resource_usage && ((checks_passed++))
    check_disk_space && ((checks_passed++))
    check_scraping_activity && ((checks_passed++))
    
    # Resumen
    local health_percentage=$((checks_passed * 100 / total_checks))
    
    if [ "$health_percentage" -eq 100 ]; then
        log_with_timestamp "âœ… Sistema saludable: $checks_passed/$total_checks verificaciones pasaron"
    elif [ "$health_percentage" -ge 80 ]; then
        log_with_timestamp "âš ï¸ Sistema mayormente saludable: $checks_passed/$total_checks verificaciones pasaron"
    else
        send_alert "Sistema con problemas: solo $checks_passed/$total_checks verificaciones pasaron" "HIGH"
    fi
    
    # EstadÃ­sticas del sistema
    log_with_timestamp "EstadÃ­sticas del sistema:"
    log_with_timestamp "- Contenedores activos: $(docker-compose ps | grep -c Up)"
    log_with_timestamp "- Uso de CPU: $(docker stats --no-stream --format '{{.CPUPerc}}' web-monitor 2>/dev/null || echo 'N/A')"
    log_with_timestamp "- Uso de Memoria: $(docker stats --no-stream --format '{{.MemUsage}}' web-monitor 2>/dev/null || echo 'N/A')"
    log_with_timestamp "- Espacio en disco: $(df -h / | awk 'NR==2 {print $5}') usado"
}

# FunciÃ³n de auto-recuperaciÃ³n
auto_recovery() {
    log_with_timestamp "Iniciando procedimientos de auto-recuperaciÃ³n..."
    
    # Intentar reiniciar servicios fallidos
    local failed_services=$(docker-compose ps --services --filter "status=exited")
    
    if [ -n "$failed_services" ]; then
        log_with_timestamp "Reiniciando servicios fallidos: $failed_services"
        echo "$failed_services" | xargs docker-compose restart
        
        # Esperar y verificar
        sleep 30
        if check_services; then
            log_with_timestamp "âœ… Auto-recuperaciÃ³n exitosa"
            send_alert "Auto-recuperaciÃ³n completada exitosamente" "INFO"
        else
            send_alert "Auto-recuperaciÃ³n fallÃ³, intervenciÃ³n manual requerida" "CRITICAL"
        fi
    fi
}

# FunciÃ³n principal
main() {
    # Crear directorio de logs si no existe
    mkdir -p "$(dirname "$LOG_FILE")"
    
    # Verificar si Docker estÃ¡ disponible
    if ! command -v docker &> /dev/null; then
        send_alert "Docker no estÃ¡ disponible en el sistema" "CRITICAL"
        exit 1
    fi
    
    # Verificar si el proyecto estÃ¡ en el directorio correcto
    if [ ! -f "docker-compose.yml" ]; then
        send_alert "docker-compose.yml no encontrado. Ejecutar desde el directorio del proyecto" "CRITICAL"
        exit 1
    fi
    
    # Ejecutar verificaciones
    run_health_checks
    
    # Auto-recuperaciÃ³n si estÃ¡ habilitada
    if [ "$1" == "--auto-recovery" ]; then
        if ! check_services || ! check_api_health; then
            auto_recovery
        fi
    fi
    
    log_with_timestamp "Monitoreo completado"
}

# Ejecutar funciÃ³n principal
main "$@"</div>
            </div>
        </div>

        <!-- Resumen Final -->
        <div class="bg-gradient-to-r from-green-500 to-blue-600 text-white p-6 rounded-lg">
            <h2 class="text-2xl font-bold mb-4">
                <i class="fas fa-check-circle mr-2"></i>WEB AUTOMÃTICO COMPETITIVO - Tier 2.5 Completado
            </h2>
            
            <div class="grid md:grid-cols-2 gap-6">
                <div>
                    <h3 class="text-lg font-semibold mb-2">ğŸ¯ CaracterÃ­sticas Implementadas</h3>
                    <ul class="text-sm space-y-1">
                        <li>âœ… Scraping inteligente con anti-detecciÃ³n</li>
                        <li>âœ… RotaciÃ³n automÃ¡tica de proxies</li>
                        <li>âœ… AnÃ¡lisis de sentimientos con IA</li>
                        <li>âœ… DetecciÃ³n de cambios competitivos</li>
                        <li>âœ… Sistema de alertas multichannel</li>
                        <li>âœ… Monitoreo 24/7 automatizado</li>
                        <li>âœ… APIs REST para integraciÃ³n</li>
                        <li>âœ… Dashboards en tiempo real</li>
                    </ul>
                </div>
                
                <div>
                    <h3 class="text-lg font-semibold mb-2">ğŸš€ TecnologÃ­as Utilizadas</h3>
                    <ul class="text-sm space-y-1">
                        <li>ğŸ Python 3.11 + FastAPI</li>
                        <li>ğŸŒ Selenium + Playwright + BeautifulSoup</li>
                        <li>ğŸ¤– Transformers + TextBlob + VADER</li>
                        <li>ğŸ—„ï¸ PostgreSQL + Redis</li>
                        <li>ğŸ³ Docker + Docker Compose</li>
                        <li>ğŸ“Š Prometheus + Grafana</li>
                        <li>âš¡ Celery + APScheduler</li>
                        <li>ğŸ”’ JWT + OAuth2 Security</li>
                    </ul>
                </div>
            </div>
            
            <div class="mt-6 p-4 bg-black bg-opacity-20 rounded-lg">
                <h3 class="font-semibold mb-2">ğŸ¬ Comandos de Inicio RÃ¡pido:</h3>
                <div class="font-mono text-sm">
                    <div>git clone &lt;repository&gt; && cd web-automatico-competitivo</div>
                    <div>chmod +x scripts/*.sh</div>
                    <div>./scripts/start.sh --test</div>
                    <div>ğŸŒ http://localhost:8080 - API Principal</div>
                </div>
            </div>
        </div>
    </div>

    <!-- Footer -->
    <div class="bg-gray-800 text-white py-6 mt-12">
        <div class="container mx-auto px-6 text-center">
            <p class="text-lg font-semibold">WEB AUTOMÃTICO COMPETITIVO Tier 2.5</p>
            <p class="text-gray-300">Business Intelligence Orchestrator v3.1 - CÃ³digo Python Completo</p>
            <p class="text-sm text-gray-400 mt-2">Sistema de monitoreo competitivo 24/7 con IA avanzada</p>
        </div>
    </div>
</body>
</html>
    <script id="html_badge_script1">
        window.__genspark_remove_badge_link = "https://www.genspark.ai/api/html_badge/" +
            "remove_badge?token=To%2FBnjzloZ3UfQdcSaYfDoSoPtiMTzbSBixnjDgPTixnk0l0UemFAN2kXe3Hq22K2t0BEnNeSt8k9i0xWa8f2%2F%2BFfd41XgiXvp%2Bdxv3IAa7mOm%2F7ONFi5W5tqKl%2BVyusQUPfplj20F1Y%2Fz3KPTeIBpLzgA6OXqH3krXuCWKJ948XRSrPM7OhK5unLmudoCmLMNesTx3ELQT3y0JgurK68oUdR7aBBk8l0c%2Be%2BHa4CeTeFW7Rd7IZJmK44Tuf3EC3Ug22x1tbbuErFLu%2FQy6NIPJC4HZXoOFDsxOlzVqcg9hBpVXBYg5E5s%2FW982%2BRxwqqiPCy%2Bvw%2BxS2vcXLntFAwpZaBrWJmqJMlCBpNF67HuVUMQLBBjFStExhLCalYTdIWr0nhmOfSgehFr8rUh2bP3Q35HmOh%2BQBwlcqXsdEcsfSRgPpc5TYXb2yCAEU07FBN6oogzdo2b%2B8wN9mYYEbAjMlD6sJjjTxr6pJBkXkB06WFXvkQiUDIimJi2pjnYAD1aSLIKt%2Bvo8YrCGsyreDyZiQS7ndD7HLtyRtM7t%2FRrs%3D";
        window.__genspark_locale = "es-ES";
        window.__genspark_token = "To/BnjzloZ3UfQdcSaYfDoSoPtiMTzbSBixnjDgPTixnk0l0UemFAN2kXe3Hq22K2t0BEnNeSt8k9i0xWa8f2/+Ffd41XgiXvp+dxv3IAa7mOm/7ONFi5W5tqKl+VyusQUPfplj20F1Y/z3KPTeIBpLzgA6OXqH3krXuCWKJ948XRSrPM7OhK5unLmudoCmLMNesTx3ELQT3y0JgurK68oUdR7aBBk8l0c+e+Ha4CeTeFW7Rd7IZJmK44Tuf3EC3Ug22x1tbbuErFLu/Qy6NIPJC4HZXoOFDsxOlzVqcg9hBpVXBYg5E5s/W982+RxwqqiPCy+vw+xS2vcXLntFAwpZaBrWJmqJMlCBpNF67HuVUMQLBBjFStExhLCalYTdIWr0nhmOfSgehFr8rUh2bP3Q35HmOh+QBwlcqXsdEcsfSRgPpc5TYXb2yCAEU07FBN6oogzdo2b+8wN9mYYEbAjMlD6sJjjTxr6pJBkXkB06WFXvkQiUDIimJi2pjnYAD1aSLIKt+vo8YrCGsyreDyZiQS7ndD7HLtyRtM7t/Rrs=";
    </script>
    
    <script id="html_notice_dialog_script" src="https://www.genspark.ai/notice_dialog.js"></script>
    