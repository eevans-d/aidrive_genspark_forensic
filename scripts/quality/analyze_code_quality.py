#!/usr/bin/env python3
"""
Code Quality Analysis & Report Generation

Analyzes:
- Code complexity (cyclomatic, cognitive)
- Code coverage
- Technical debt
- Code duplication
- Type safety
- Security issues in code
"""

import os
import json
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple
from dataclasses import dataclass, asdict
from enum import Enum

class MetricType(Enum):
    COMPLEXITY = "complexity"
    COVERAGE = "coverage"
    DUPLICATION = "duplication"
    MAINTAINABILITY = "maintainability"
    SECURITY = "security"

@dataclass
class CodeMetric:
    metric_type: MetricType
    name: str
    value: float
    target: float
    unit: str
    status: str  # pass, warning, fail

class CodeQualityAnalyzer:
    """Comprehensive code quality analysis"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = project_root
        self.metrics: List[CodeMetric] = []
        self.issues: Dict[str, Dict] = {}
    
    def log(self, message: str):
        """Print message"""
        print(f"[{datetime.now():%H:%M:%S}] {message}")
    
    def run_command(self, cmd: str) -> Tuple[int, str, str]:
        """Execute command"""
        try:
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
            return result.returncode, result.stdout, result.stderr
        except subprocess.TimeoutExpired:
            return 1, "", "Timeout"
        except Exception as e:
            return 1, "", str(e)
    
    # === PYLINT ANALYSIS ===
    
    def analyze_pylint(self) -> Dict:
        """Run pylint analysis"""
        self.log("üîç Running pylint analysis...")
        
        # Find Python files
        returncode, stdout, _ = self.run_command(
            f"find {self.project_root} -name '*.py' -type f | grep -v __pycache__ | grep -v '.venv'"
        )
        
        python_files = [f for f in stdout.split('\n') if f.strip() and f.endswith('.py')]
        
        if not python_files:
            self.log("‚ö†Ô∏è  No Python files found")
            return {}
        
        # Run pylint
        returncode, stdout, _ = self.run_command(
            f"pylint {' '.join(python_files[:10])} --output-format=json 2>/dev/null || echo '[]'"
        )
        
        try:
            issues = json.loads(stdout) if stdout.strip() else []
            
            # Categorize issues
            categories = {}
            for issue in issues:
                msg_type = issue.get('type', 'unknown')
                if msg_type not in categories:
                    categories[msg_type] = []
                categories[msg_type].append(issue['message'])
            
            self.issues['pylint'] = categories
            
            return {
                "tool": "pylint",
                "total_issues": len(issues),
                "categories": categories,
                "files_analyzed": len(python_files)
            }
        
        except json.JSONDecodeError:
            return {}
    
    # === COMPLEXITY ANALYSIS ===
    
    def analyze_complexity(self) -> List[CodeMetric]:
        """Analyze cyclomatic complexity"""
        self.log("üîç Analyzing cyclomatic complexity...")
        
        # Find high complexity functions
        returncode, stdout, _ = self.run_command(
            "radon cc --average -s " + self.project_root + " 2>/dev/null || echo 'N/A'"
        )
        
        metrics = []
        
        # Parse radon output
        if stdout and "N/A" not in stdout:
            lines = stdout.split('\n')
            for line in lines:
                if "Average" in line or "Complexity" in line:
                    # Extract values
                    try:
                        value = float(line.split(':')[-1].strip())
                        status = "pass" if value < 5 else "warning" if value < 8 else "fail"
                        
                        metrics.append(CodeMetric(
                            metric_type=MetricType.COMPLEXITY,
                            name="Average Complexity",
                            value=value,
                            target=5.0,
                            unit="",
                            status=status
                        ))
                    except:
                        pass
        
        return metrics
    
    # === COVERAGE ANALYSIS ===
    
    def analyze_coverage(self) -> List[CodeMetric]:
        """Analyze test coverage"""
        self.log("üîç Analyzing test coverage...")
        
        # Run coverage
        returncode, stdout, stderr = self.run_command(
            "cd " + self.project_root + " && python -m pytest --cov --cov-report=json 2>/dev/null"
        )
        
        metrics = []
        
        # Parse coverage report
        coverage_file = os.path.join(self.project_root, ".coverage")
        if os.path.exists(coverage_file):
            returncode, stdout, _ = self.run_command(
                "coverage report --skip-covered 2>/dev/null | tail -1"
            )
            
            if stdout:
                try:
                    # Extract percentage from coverage report
                    percentage = float(stdout.split()[-1].rstrip('%'))
                    status = "pass" if percentage >= 85 else "warning" if percentage >= 75 else "fail"
                    
                    metrics.append(CodeMetric(
                        metric_type=MetricType.COVERAGE,
                        name="Test Coverage",
                        value=percentage,
                        target=85.0,
                        unit="%",
                        status=status
                    ))
                except:
                    pass
        
        return metrics
    
    # === DUPLICATION ANALYSIS ===
    
    def analyze_duplication(self) -> List[CodeMetric]:
        """Analyze code duplication"""
        self.log("üîç Analyzing code duplication...")
        
        # Run CPD (Copy-Paste Detector)
        returncode, stdout, stderr = self.run_command(
            f"find {self.project_root} -name '*.py' -type f | head -20 | xargs wc -l | tail -1"
        )
        
        metrics = []
        
        if stdout:
            try:
                total_lines = int(stdout.split()[0])
                
                # Estimate duplication (would need proper CPD)
                duplication_percent = 0  # Placeholder
                
                metrics.append(CodeMetric(
                    metric_type=MetricType.DUPLICATION,
                    name="Code Duplication",
                    value=duplication_percent,
                    target=5.0,
                    unit="%",
                    status="pass" if duplication_percent < 10 else "warning"
                ))
            except:
                pass
        
        return metrics
    
    # === MAINTAINABILITY INDEX ===
    
    def analyze_maintainability(self) -> List[CodeMetric]:
        """Calculate maintainability index"""
        self.log("üîç Calculating maintainability index...")
        
        # Run radon mi (maintainability index)
        returncode, stdout, _ = self.run_command(
            f"radon mi {self.project_root} -s -j 2>/dev/null | grep -o '\"mi\": [0-9.]*' | head -1"
        )
        
        metrics = []
        
        if stdout:
            try:
                mi_value = float(stdout.split(':')[1].strip())
                
                # MI scale: 0-100
                # > 85: High maintainability
                # 65-85: Medium maintainability
                # < 65: Low maintainability
                
                if mi_value >= 85:
                    status = "pass"
                elif mi_value >= 65:
                    status = "warning"
                else:
                    status = "fail"
                
                metrics.append(CodeMetric(
                    metric_type=MetricType.MAINTAINABILITY,
                    name="Maintainability Index",
                    value=mi_value,
                    target=80.0,
                    unit="",
                    status=status
                ))
            except:
                pass
        
        return metrics
    
    # === SECURITY ANALYSIS ===
    
    def analyze_security_code(self) -> List[CodeMetric]:
        """Analyze code for security issues"""
        self.log("üîç Analyzing code security...")
        
        # Run bandit
        returncode, stdout, stderr = self.run_command(
            f"bandit -r {self.project_root} -f json 2>/dev/null | grep -o '\"severity\": \"[^\"]*\"' | wc -l"
        )
        
        metrics = []
        
        try:
            issue_count = int(stdout.strip()) if stdout else 0
            status = "pass" if issue_count == 0 else "warning" if issue_count < 5 else "fail"
            
            metrics.append(CodeMetric(
                metric_type=MetricType.SECURITY,
                name="Security Issues Found",
                value=float(issue_count),
                target=0.0,
                unit="",
                status=status
            ))
        except:
            pass
        
        return metrics
    
    # === TYPE CHECKING ===
    
    def analyze_type_hints(self) -> Dict:
        """Analyze type hint coverage"""
        self.log("üîç Analyzing type hint coverage...")
        
        # Run mypy
        returncode, stdout, _ = self.run_command(
            f"mypy {self.project_root} --json 2>/dev/null | head -20"
        )
        
        return {
            "tool": "mypy",
            "output": stdout[:500] if stdout else "No output"
        }
    
    def run_all_analysis(self):
        """Run all code quality analysis"""
        self.log("‚ïî" + "=" * 50 + "‚ïó")
        self.log("‚ïë  CODE QUALITY ANALYSIS STARTED                ‚ïë")
        self.log("‚ïö" + "=" * 50 + "‚ïù")
        self.log("")
        
        # Collect all metrics
        self.metrics.extend(self.analyze_complexity())
        self.metrics.extend(self.analyze_coverage())
        self.metrics.extend(self.analyze_duplication())
        self.metrics.extend(self.analyze_maintainability())
        self.metrics.extend(self.analyze_security_code())
        
        # Additional analysis
        pylint_result = self.analyze_pylint()
        type_hints = self.analyze_type_hints()
        
        return {
            "metrics": self.metrics,
            "issues": self.issues,
            "pylint": pylint_result,
            "type_hints": type_hints
        }
    
    def generate_report(self, results: Dict):
        """Generate comprehensive code quality report"""
        print("\n" + "=" * 70)
        print("CODE QUALITY ANALYSIS REPORT")
        print("=" * 70)
        print(f"Generated: {datetime.now()}")
        print("")
        
        # Summary metrics
        print("üìä KEY METRICS:")
        print("-" * 70)
        
        for metric in self.metrics:
            status_icon = "‚úÖ" if metric.status == "pass" else "‚ö†Ô∏è" if metric.status == "warning" else "‚ùå"
            print(f"{status_icon} {metric.name}: {metric.value:.1f}{metric.unit} (target: {metric.target}{metric.unit})")
        
        print("")
        print("üêõ ISSUES FOUND:")
        print("-" * 70)
        
        for tool, issues in self.issues.items():
            print(f"\n{tool.upper()}:")
            for category, items in issues.items():
                print(f"  {category}: {len(items)} issues")
        
        # Quality score
        passed = sum(1 for m in self.metrics if m.status == "pass")
        total = len(self.metrics)
        score = (passed / total * 100) if total > 0 else 0
        
        print("")
        print(f"OVERALL CODE QUALITY SCORE: {score:.0f}%")
        
        # Recommendations
        print("")
        print("üìã RECOMMENDATIONS:")
        print("-" * 70)
        
        for metric in self.metrics:
            if metric.status != "pass":
                print(f"‚ö†Ô∏è  {metric.name}: {metric.value:.1f} (target: {metric.target})")
                
                if metric.metric_type == MetricType.COMPLEXITY:
                    print("   ‚Üí Refactor functions with high complexity")
                    print("   ‚Üí Break down complex logic into smaller functions")
                
                elif metric.metric_type == MetricType.COVERAGE:
                    print("   ‚Üí Add unit tests for uncovered code")
                    print("   ‚Üí Target: 92%+ coverage")
                
                elif metric.metric_type == MetricType.DUPLICATION:
                    print("   ‚Üí Extract common code into utility functions")
                    print("   ‚Üí Use inheritance/composition patterns")
                
                elif metric.metric_type == MetricType.MAINTAINABILITY:
                    print("   ‚Üí Simplify complex modules")
                    print("   ‚Üí Improve documentation")
                    print("   ‚Üí Reduce cyclomatic complexity")
                
                elif metric.metric_type == MetricType.SECURITY:
                    print("   ‚Üí Review flagged security issues")
                    print("   ‚Üí Update dependencies")
        
        # Save JSON report
        report = {
            "timestamp": datetime.now().isoformat(),
            "metrics": [asdict(m) for m in self.metrics],
            "score": score,
            "total_metrics": total,
            "passed": passed
        }
        
        report_file = f"/var/log/code-quality-reports/analysis_{datetime.now():%Y%m%d_%H%M%S}.json"
        os.makedirs(os.path.dirname(report_file), exist_ok=True)
        
        with open(report_file, "w") as f:
            json.dump(report, f, indent=2)
        
        print(f"\nReport saved: {report_file}")

if __name__ == "__main__":
    analyzer = CodeQualityAnalyzer(project_root=".")
    results = analyzer.run_all_analysis()
    analyzer.generate_report(results)
