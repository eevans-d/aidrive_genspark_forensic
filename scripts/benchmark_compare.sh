#!/bin/bash
# Compare benchmarks before and after optimizations

set -e

BEFORE=""
AFTER=""
HELP=false

# Parse arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --before=*)
            BEFORE="${1#*=}"
            shift
            ;;
        --after=*)
            AFTER="${1#*=}"
            shift
            ;;
        -h|--help)
            HELP=true
            shift
            ;;
        *)
            echo "Unknown parameter: $1"
            exit 1
            ;;
    esac
done

if [ "$HELP" = true ]; then
    echo "Usage: $0 --before=<baseline> --after=<current>"
    echo ""
    echo "Options:"
    echo "  --before=NAME  Baseline benchmark identifier"
    echo "  --after=NAME   Current benchmark identifier"  
    echo "  -h, --help     Show this help"
    exit 0
fi

if [ -z "$BEFORE" ] || [ -z "$AFTER" ]; then
    echo "Error: Both --before and --after parameters are required"
    exit 1
fi

echo "ðŸ“Š Benchmark Comparison: $BEFORE vs $AFTER"
echo "=========================================="

BENCHMARK_DIR="docs/benchmarks"
mkdir -p "$BENCHMARK_DIR"

DATE=$(date +%Y%m%d_%H%M)
COMPARISON_REPORT="$BENCHMARK_DIR/comparison_${BEFORE}_vs_${AFTER}_${DATE}.md"

# Initialize comparison report
cat > "$COMPARISON_REPORT" << EOF
# Benchmark Comparison Report
Date: $(date)
Baseline: $BEFORE
Current: $AFTER

## Summary
EOF

# Function to run simple performance test
run_performance_test() {
    local test_name="$1"
    local test_dir="$2"
    
    echo "ðŸ” Testing performance: $test_name"
    
    if [ ! -d "$test_dir" ]; then
        echo "  âš ï¸  Directory not found: $test_dir"
        return 1
    fi
    
    cd "$test_dir"
    
    # Simple Python import time test
    if find . -name "*.py" | head -1 | grep -q .; then
        MAIN_PY=$(find . -name "*.py" | head -1)
        
        echo "  ðŸ“¦ Testing import time for: $MAIN_PY"
        
        # Measure import time
        IMPORT_TIME=$(python -c "
import time
start = time.time()
try:
    import sys
    sys.path.append('.')
    # Try to import the main module
    print('Import successful')
except Exception as e:
    print(f'Import failed: {e}')
end = time.time()
print(f'Time: {end - start:.3f}s')
" 2>&1 | grep "Time:" | cut -d' ' -f2 || echo "N/A")
        
        echo "  â±ï¸  Import time: $IMPORT_TIME"
        
        # Store result
        echo "### $test_name" >> "$COMPARISON_REPORT"
        echo "- Import time: $IMPORT_TIME" >> "$COMPARISON_REPORT"
        echo "" >> "$COMPARISON_REPORT"
    fi
    
    cd - > /dev/null
}

# Function to analyze code metrics
analyze_code_metrics() {
    local module_dir="$1"
    local module_name="$2"
    
    echo "ðŸ“ˆ Analyzing code metrics: $module_name"
    
    if [ ! -d "$module_dir" ]; then
        echo "  âš ï¸  Directory not found: $module_dir"
        return 1
    fi
    
    # Count Python files
    PY_FILES=$(find "$module_dir" -name "*.py" | wc -l)
    
    # Count lines of code
    LOC=$(find "$module_dir" -name "*.py" -exec wc -l {} + 2>/dev/null | tail -1 | awk '{print $1}' || echo "0")
    
    # Count functions/classes (rough estimate)
    FUNCTIONS=$(grep -r "^def " "$module_dir" --include="*.py" | wc -l 2>/dev/null || echo "0")
    CLASSES=$(grep -r "^class " "$module_dir" --include="*.py" | wc -l 2>/dev/null || echo "0")
    
    echo "  ðŸ“Š Files: $PY_FILES, LOC: $LOC, Functions: $FUNCTIONS, Classes: $CLASSES"
    
    # Store metrics
    echo "### $module_name Code Metrics" >> "$COMPARISON_REPORT"
    echo "- Python files: $PY_FILES" >> "$COMPARISON_REPORT"
    echo "- Lines of code: $LOC" >> "$COMPARISON_REPORT" 
    echo "- Functions: $FUNCTIONS" >> "$COMPARISON_REPORT"
    echo "- Classes: $CLASSES" >> "$COMPARISON_REPORT"
    echo "" >> "$COMPARISON_REPORT"
}

# Function to check for optimization indicators
check_optimizations() {
    echo "ðŸ”§ Checking for optimization indicators"
    
    # Database optimizations
    SQLITE_CONFIGS=$(find . -name "*sqlite_config*" | wc -l)
    
    # Caching implementations
    CACHE_FILES=$(find . -name "*cache*" | wc -l)
    
    # Performance decorators
    DECORATORS=$(grep -r "@.*performance\|@.*cache\|@.*memoize" . --include="*.py" | wc -l 2>/dev/null || echo "0")
    
    # Metrics endpoints
    METRICS=$(grep -r "/metrics\|prometheus\|monitoring" . --include="*.py" | wc -l 2>/dev/null || echo "0")
    
    echo "  ðŸ—„ï¸  SQLite configs: $SQLITE_CONFIGS"
    echo "  ðŸš€ Cache files: $CACHE_FILES"  
    echo "  âš¡ Performance decorators: $DECORATORS"
    echo "  ðŸ“Š Metrics indicators: $METRICS"
    
    # Store optimization metrics
    echo "### Optimization Indicators" >> "$COMPARISON_REPORT"
    echo "- SQLite configurations: $SQLITE_CONFIGS" >> "$COMPARISON_REPORT"
    echo "- Cache implementations: $CACHE_FILES" >> "$COMPARISON_REPORT"
    echo "- Performance decorators: $DECORATORS" >> "$COMPARISON_REPORT" 
    echo "- Metrics endpoints: $METRICS" >> "$COMPARISON_REPORT"
    echo "" >> "$COMPARISON_REPORT"
}

# Run performance tests for each module
echo ""
echo "ðŸš€ Running Performance Tests..."
run_performance_test "Inventario Retail" "inventario-retail"
run_performance_test "BI Orchestrator" "business-intelligence-orchestrator-v3.1" 
run_performance_test "Sistema Deposito" "sistema_deposito_semana1"

echo ""
echo "ðŸ“Š Analyzing Code Metrics..."
analyze_code_metrics "inventario-retail" "Inventario Retail"
analyze_code_metrics "business-intelligence-orchestrator-v3.1" "BI Orchestrator"
analyze_code_metrics "sistema_deposito_semana1" "Sistema Deposito"

echo ""
check_optimizations

# Add improvement recommendations
cat >> "$COMPARISON_REPORT" << EOF

## Recommendations

### Performance Improvements Detected
EOF

# Check if optimizations were implemented
if [ "$AFTER" = "current" ]; then
    if find . -name "*sqlite_config*" | grep -q .; then
        echo "- âœ… Database configuration optimized" >> "$COMPARISON_REPORT"
    else
        echo "- â³ Database optimization opportunity" >> "$COMPARISON_REPORT"
    fi
    
    if grep -r "cache" . --include="*.py" > /dev/null 2>&1; then
        echo "- âœ… Caching mechanisms implemented" >> "$COMPARISON_REPORT"
    else
        echo "- â³ Caching opportunity identified" >> "$COMPARISON_REPORT"
    fi
    
    if grep -r "/metrics" . --include="*.py" > /dev/null 2>&1; then
        echo "- âœ… Performance monitoring enabled" >> "$COMPARISON_REPORT"  
    else
        echo "- â³ Monitoring setup recommended" >> "$COMPARISON_REPORT"
    fi
fi

cat >> "$COMPARISON_REPORT" << EOF

### Next Steps
1. Focus on modules with highest complexity
2. Implement caching for frequent operations
3. Add monitoring for critical paths
4. Regular performance regression testing

---
*Generated by benchmark_compare.sh on $(date)*
EOF

echo ""
echo "âœ… Benchmark comparison completed!"
echo "ðŸ“‹ Report generated: $COMPARISON_REPORT"
echo ""
echo "ðŸ“Š Summary:"
echo "- Performance tests completed for 3 modules"
echo "- Code metrics analyzed"
echo "- Optimization indicators checked"
echo "- Recommendations provided"
echo ""
echo "ðŸ” View the full report:"
echo "cat $COMPARISON_REPORT"