#!/usr/bin/env python3
"""
üöÄ MEGA AN√ÅLISIS - FASE 4: Optimizaci√≥n de Performance Cr√≠tica
An√°lisis exhaustivo de performance para refactoring enterprise de archivos monol√≠ticos.

Target: Reducir memoria 596MB‚Üí<300MB, throughput 213‚Üí1,000 req/seg, accuracy 92.90%‚Üí95%+
"""

import os
import re
import json
import ast
from typing import Dict, List, Any, Tuple
from datetime import datetime
from pathlib import Path
import hashlib

class PerformanceAnalyzer:
    def __init__(self):
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "analysis_type": "performance_critical_optimization",
            "targets": {
                "memory_reduction": {"current": "596MB", "target": "<300MB", "improvement": "50%+"},
                "throughput_increase": {"current": "213 req/seg", "target": "1,000 req/seg", "improvement": "370%+"},
                "accuracy_improvement": {"current": "92.90%", "target": "95%+", "improvement": "2.1%+"},
                "response_time": {"current": "1800ms", "target": "<1000ms", "improvement": "44%+"}
            },
            "files_analyzed": [],
            "performance_issues": [],
            "memory_analysis": {},
            "throughput_bottlenecks": {},
            "caching_opportunities": [],
            "refactoring_plan": {}
        }
        
    def analyze_memory_patterns(self, content: str, file_path: str) -> Dict[str, Any]:
        """Analiza patrones que consumen memoria excesiva"""
        memory_issues = {
            "large_arrays": [],
            "string_concatenations": [],
            "closures": [],
            "memory_leaks": [],
            "inefficient_loops": [],
            "memory_score": 0
        }
        
        lines = content.split('\n')
        
        for i, line in enumerate(lines, 1):
            line_stripped = line.strip()
            
            # Arrays grandes sin paginaci√≥n
            if re.search(r'\.map\(|\.filter\(|\.reduce\(', line_stripped):
                if 'length' in line_stripped or 'size' in line_stripped:
                    memory_issues["large_arrays"].append({
                        "line": i,
                        "content": line_stripped[:100],
                        "issue": "Operaci√≥n en array grande sin paginaci√≥n",
                        "impact": "HIGH"
                    })
            
            # Concatenaci√≥n de strings ineficiente
            if '+=' in line_stripped and ('string' in line_stripped.lower() or '"' in line_stripped):
                memory_issues["string_concatenations"].append({
                    "line": i,
                    "content": line_stripped[:100],
                    "issue": "Concatenaci√≥n de string ineficiente (usar StringBuilder pattern)",
                    "impact": "MEDIUM"
                })
            
            # Closures que pueden causar memory leaks
            if re.search(r'function.*\{.*\}.*\)', line_stripped) or 'setInterval' in line_stripped or 'setTimeout' in line_stripped:
                memory_issues["closures"].append({
                    "line": i,
                    "content": line_stripped[:100],
                    "issue": "Closure potencial memory leak",
                    "impact": "HIGH"
                })
            
            # Loops ineficientes
            if re.search(r'for\s*\(.*\.length.*\)', line_stripped):
                memory_issues["inefficient_loops"].append({
                    "line": i,
                    "content": line_stripped[:100],
                    "issue": "Loop accede .length en cada iteraci√≥n",
                    "impact": "MEDIUM"
                })
            
            # Variables no liberadas
            if 'const' not in line_stripped and ('let ' in line_stripped or 'var ' in line_stripped):
                if any(keyword in line_stripped for keyword in ['fetch', 'axios', 'request', 'response']):
                    memory_issues["memory_leaks"].append({
                        "line": i,
                        "content": line_stripped[:100],
                        "issue": "Variable HTTP no const, posible memory leak",
                        "impact": "MEDIUM"
                    })
        
        # Calcular score de memoria
        total_issues = sum(len(issues) for issues in memory_issues.values() if isinstance(issues, list))
        memory_issues["memory_score"] = max(0, 10 - (total_issues * 0.5))
        
        return memory_issues
    
    def analyze_throughput_bottlenecks(self, content: str, file_path: str) -> Dict[str, Any]:
        """Identifica bottlenecks que limitan throughput"""
        bottlenecks = {
            "synchronous_operations": [],
            "blocking_io": [],
            "inefficient_queries": [],
            "missing_parallelization": [],
            "rate_limiting_issues": [],
            "throughput_score": 0
        }
        
        lines = content.split('\n')
        
        for i, line in enumerate(lines, 1):
            line_stripped = line.strip()
            
            # Operaciones s√≠ncronas que deber√≠an ser async
            if any(sync_op in line_stripped for sync_op in ['fs.readFileSync', 'fs.writeFileSync', 'JSON.parse(']):
                bottlenecks["synchronous_operations"].append({
                    "line": i,
                    "content": line_stripped[:100],
                    "issue": "Operaci√≥n s√≠ncrona bloquea event loop",
                    "impact": "CRITICAL"
                })
            
            # Blocking I/O operations
            if 'await' in line_stripped and not 'Promise.all' in line_stripped:
                if any(io_op in line_stripped for io_op in ['fetch', 'supabase', 'query', 'insert', 'update']):
                    bottlenecks["blocking_io"].append({
                        "line": i,
                        "content": line_stripped[:100],
                        "issue": "I/O operation no paralellizada",
                        "impact": "HIGH"
                    })
            
            # Queries ineficientes
            if 'select' in line_stripped.lower() and '*' in line_stripped:
                bottlenecks["inefficient_queries"].append({
                    "line": i,
                    "content": line_stripped[:100],
                    "issue": "SELECT * ineficiente",
                    "impact": "HIGH"
                })
            
            # Missing parallelization opportunities
            if 'for' in line_stripped and 'await' in line_stripped:
                if 'Promise.all' not in content[max(0, content.find(line_stripped)-500):content.find(line_stripped)+500]:
                    bottlenecks["missing_parallelization"].append({
                        "line": i,
                        "content": line_stripped[:100],
                        "issue": "Loop con await secuencial, deber√≠a usar Promise.all",
                        "impact": "CRITICAL"
                    })
            
            # Rate limiting issues
            if any(rate_term in line_stripped for rate_term in ['sleep', 'delay', 'setTimeout']):
                if any(scrape_term in line_stripped for scrape_term in ['scrape', 'fetch', 'request']):
                    bottlenecks["rate_limiting_issues"].append({
                        "line": i,
                        "content": line_stripped[:100],
                        "issue": "Rate limiting agresivo impacta throughput",
                        "impact": "MEDIUM"
                    })
        
        # Calcular score de throughput
        critical_issues = len(bottlenecks["synchronous_operations"]) + len(bottlenecks["missing_parallelization"])
        high_issues = len(bottlenecks["blocking_io"]) + len(bottlenecks["inefficient_queries"])
        
        bottlenecks["throughput_score"] = max(0, 10 - (critical_issues * 2) - (high_issues * 1))
        
        return bottlenecks
    
    def analyze_accuracy_issues(self, content: str, file_path: str) -> Dict[str, Any]:
        """Analiza patrones que afectan accuracy del scraping"""
        accuracy_issues = {
            "error_handling": [],
            "retry_logic": [],
            "data_validation": [],
            "selector_robustness": [],
            "accuracy_score": 0
        }
        
        lines = content.split('\n')
        
        for i, line in enumerate(lines, 1):
            line_stripped = line.strip()
            
            # Error handling d√©bil
            if 'catch' in line_stripped:
                if 'console.log' in line_stripped or line_stripped.count('{') == line_stripped.count('}'):
                    accuracy_issues["error_handling"].append({
                        "line": i,
                        "content": line_stripped[:100],
                        "issue": "Error handling d√©bil, solo console.log",
                        "impact": "HIGH"
                    })
            
            # Missing retry logic
            if any(http_term in line_stripped for http_term in ['fetch', 'axios', 'request']):
                context_lines = lines[max(0, i-3):i+3]
                if not any('retry' in line.lower() for line in context_lines):
                    accuracy_issues["retry_logic"].append({
                        "line": i,
                        "content": line_stripped[:100],
                        "issue": "HTTP request sin retry logic",
                        "impact": "HIGH"
                    })
            
            # Data validation d√©bil
            if any(data_term in line_stripped for data_term in ['price', 'producto', 'item', 'data']):
                if not any(validation in line_stripped for validation in ['isNaN', 'typeof', 'parseInt', 'parseFloat']):
                    accuracy_issues["data_validation"].append({
                        "line": i,
                        "content": line_stripped[:100],
                        "issue": "Data extraction sin validaci√≥n",
                        "impact": "MEDIUM"
                    })
            
            # Selectores poco robustos
            if any(selector_term in line_stripped for selector_term in ['querySelector', 'getElementsBy', '$(']):
                if not any(fallback in line_stripped for fallback in ['||', '??', 'try']):
                    accuracy_issues["selector_robustness"].append({
                        "line": i,
                        "content": line_stripped[:100],
                        "issue": "Selector sin fallback",
                        "impact": "HIGH"
                    })
        
        # Calcular accuracy score
        total_issues = sum(len(issues) for issues in accuracy_issues.values() if isinstance(issues, list))
        accuracy_issues["accuracy_score"] = max(0, 10 - (total_issues * 0.3))
        
        return accuracy_issues
    
    def identify_caching_opportunities(self, content: str, file_path: str) -> List[Dict[str, Any]]:
        """Identifica oportunidades de caching"""
        opportunities = []
        lines = content.split('\n')
        
        for i, line in enumerate(lines, 1):
            line_stripped = line.strip()
            
            # API calls repetitivos
            if any(api_term in line_stripped for api_term in ['fetch', 'supabase.from', 'query']):
                if any(static_term in line_stripped for static_term in ['productos', 'categorias', 'config']):
                    opportunities.append({
                        "line": i,
                        "type": "API_CACHING",
                        "content": line_stripped[:100],
                        "opportunity": "Cache API calls de datos est√°ticos",
                        "impact": "HIGH",
                        "estimated_improvement": "60% reduction en DB calls"
                    })
            
            # Computaciones costosas
            if any(compute_term in line_stripped for compute_term in ['parse', 'calculate', 'process', 'transform']):
                if 'for' in line_stripped or 'map' in line_stripped:
                    opportunities.append({
                        "line": i,
                        "type": "COMPUTATION_CACHING",
                        "content": line_stripped[:100],
                        "opportunity": "Cache resultados de computaciones costosas",
                        "impact": "MEDIUM",
                        "estimated_improvement": "40% reduction en CPU usage"
                    })
            
            # File operations repetitivas
            if any(file_term in line_stripped for file_term in ['readFile', 'writeFile', 'import', 'require']):
                opportunities.append({
                    "line": i,
                    "type": "FILE_CACHING",
                    "content": line_stripped[:100],
                    "opportunity": "Cache file operations",
                    "impact": "MEDIUM",
                    "estimated_improvement": "50% reduction en I/O wait"
                })
        
        return opportunities
    
    def generate_refactoring_plan(self, file_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Genera plan de refactoring espec√≠fico"""
        plan = {
            "priority_order": [],
            "modules_to_extract": [],
            "performance_optimizations": [],
            "caching_strategy": {},
            "estimated_impact": {},
            "implementation_phases": []
        }
        
        # Calcular prioridades basadas en impacto
        critical_files = []
        high_impact_files = []
        
        for file_path, analysis in file_analysis.items():
            if analysis.get("lines", 0) > 2000:  # Archivos monol√≠ticos
                critical_score = 0
                if analysis.get("memory_analysis", {}).get("memory_score", 10) < 5:
                    critical_score += 3
                if analysis.get("throughput_analysis", {}).get("throughput_score", 10) < 5:
                    critical_score += 3
                if analysis.get("accuracy_analysis", {}).get("accuracy_score", 10) < 7:
                    critical_score += 2
                
                if critical_score >= 6:
                    critical_files.append(file_path)
                elif critical_score >= 3:
                    high_impact_files.append(file_path)
        
        plan["priority_order"] = critical_files + high_impact_files
        
        # Modules to extract
        for file_path in critical_files:
            analysis = file_analysis[file_path]
            suggested_modules = []
            
            if analysis.get("lines", 0) > 3000:
                suggested_modules.extend([
                    f"{Path(file_path).stem}_auth",
                    f"{Path(file_path).stem}_validation", 
                    f"{Path(file_path).stem}_utils",
                    f"{Path(file_path).stem}_cache"
                ])
            
            plan["modules_to_extract"].append({
                "file": file_path,
                "suggested_modules": suggested_modules,
                "estimated_reduction": f"{len(suggested_modules) * 300}-{len(suggested_modules) * 500} lines"
            })
        
        # Performance optimizations espec√≠ficas
        plan["performance_optimizations"] = [
            {
                "type": "MEMORY_OPTIMIZATION",
                "actions": [
                    "Implementar object pooling para requests recurrentes",
                    "Usar WeakMap para referencias temporales",
                    "Implement streaming para large datasets",
                    "Optimize garbage collection con --max-old-space-size"
                ],
                "estimated_memory_reduction": "200-300MB"
            },
            {
                "type": "THROUGHPUT_OPTIMIZATION", 
                "actions": [
                    "Implementar Promise.all para operaciones paralelas",
                    "Connection pooling para database",
                    "Batch processing para multiple inserts",
                    "Implement queue system para rate limiting inteligente"
                ],
                "estimated_throughput_increase": "400-600%"
            },
            {
                "type": "ACCURACY_OPTIMIZATION",
                "actions": [
                    "Robust error handling con exponential backoff",
                    "Multi-selector fallback chains",
                    "Data validation layers",
                    "Implement data quality scoring"
                ],
                "estimated_accuracy_increase": "2.5-3.5%"
            }
        ]
        
        # Caching strategy
        plan["caching_strategy"] = {
            "layers": [
                {
                    "level": "APPLICATION", 
                    "type": "In-memory LRU cache",
                    "data": "API responses, computed results",
                    "ttl": "5-15 minutes"
                },
                {
                    "level": "DATABASE",
                    "type": "Query result caching", 
                    "data": "Static data, product catalogs",
                    "ttl": "1-6 hours"
                },
                {
                    "level": "HTTP",
                    "type": "Response caching",
                    "data": "Static assets, API responses",
                    "ttl": "30 minutes - 24 hours"
                }
            ],
            "estimated_performance_gain": "300-500% response time improvement"
        }
        
        # Implementation phases
        plan["implementation_phases"] = [
            {
                "phase": 1,
                "name": "Critical Refactoring",
                "duration": "1-2 weeks",
                "focus": "Split monolithic files, implement basic caching",
                "expected_gains": "Memory: -200MB, Throughput: +200%"
            },
            {
                "phase": 2, 
                "name": "Performance Optimization",
                "duration": "1-2 weeks",
                "focus": "Parallel processing, connection pooling, optimization",
                "expected_gains": "Throughput: +300%, Response time: -40%"
            },
            {
                "phase": 3,
                "name": "Advanced Optimization",
                "duration": "1 week", 
                "focus": "Advanced caching, accuracy improvements",
                "expected_gains": "Accuracy: +2.5%, Memory: additional -100MB"
            }
        ]
        
        # Estimated total impact
        plan["estimated_impact"] = {
            "memory_reduction": "300-400MB (50-67% reduction)",
            "throughput_increase": "500-800% (213 ‚Üí 1,000+ req/seg)",
            "accuracy_improvement": "2.5-3.5% (92.90% ‚Üí 95.5%+)",
            "response_time": "1800ms ‚Üí 600-800ms (56-67% improvement)",
            "roi": "800-1200% in 3-6 months"
        }
        
        return plan
    
    def analyze_file(self, file_path: str) -> Dict[str, Any]:
        """Analiza un archivo espec√≠fico"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            lines = content.split('\n')
            
            analysis = {
                "file_path": file_path,
                "lines": len(lines),
                "size_kb": len(content) / 1024,
                "memory_analysis": self.analyze_memory_patterns(content, file_path),
                "throughput_analysis": self.analyze_throughput_bottlenecks(content, file_path),
                "accuracy_analysis": self.analyze_accuracy_issues(content, file_path),
                "caching_opportunities": self.identify_caching_opportunities(content, file_path),
                "overall_performance_score": 0
            }
            
            # Calcular score overall
            memory_score = analysis["memory_analysis"].get("memory_score", 0)
            throughput_score = analysis["throughput_analysis"].get("throughput_score", 0)
            accuracy_score = analysis["accuracy_analysis"].get("accuracy_score", 0)
            
            analysis["overall_performance_score"] = (memory_score * 0.4 + throughput_score * 0.4 + accuracy_score * 0.2)
            
            return analysis
            
        except Exception as e:
            return {
                "file_path": file_path,
                "error": str(e),
                "overall_performance_score": 0
            }
    
    def run_analysis(self, target_files: List[str]) -> Dict[str, Any]:
        """Ejecuta an√°lisis completo"""
        print("üöÄ INICIANDO MEGA AN√ÅLISIS - FASE 4: Optimizaci√≥n de Performance Cr√≠tica")
        print("=" * 80)
        
        file_analysis = {}
        
        for file_path in target_files:
            if os.path.exists(file_path):
                print(f"üìä Analizando: {file_path}")
                analysis = self.analyze_file(file_path)
                file_analysis[file_path] = analysis
                self.results["files_analyzed"].append(file_path)
                
                # Summary de issues por archivo
                memory_issues = sum(len(v) for v in analysis.get("memory_analysis", {}).values() if isinstance(v, list))
                throughput_issues = sum(len(v) for v in analysis.get("throughput_analysis", {}).values() if isinstance(v, list))
                accuracy_issues = sum(len(v) for v in analysis.get("accuracy_analysis", {}).values() if isinstance(v, list))
                
                print(f"  üìà Performance Score: {analysis['overall_performance_score']:.1f}/10")
                print(f"  üß† Memory Issues: {memory_issues}")
                print(f"  ‚ö° Throughput Issues: {throughput_issues}")
                print(f"  üéØ Accuracy Issues: {accuracy_issues}")
                print(f"  üíæ Caching Opportunities: {len(analysis.get('caching_opportunities', []))}")
                print("-" * 50)
            else:
                print(f"‚ùå Archivo no encontrado: {file_path}")
        
        # Generar plan de refactoring
        self.results["refactoring_plan"] = self.generate_refactoring_plan(file_analysis)
        
        # Consolidar resultados
        total_memory_issues = sum(
            sum(len(v) for v in analysis.get("memory_analysis", {}).values() if isinstance(v, list))
            for analysis in file_analysis.values()
        )
        
        total_throughput_issues = sum(
            sum(len(v) for v in analysis.get("throughput_analysis", {}).values() if isinstance(v, list))
            for analysis in file_analysis.values()
        )
        
        total_accuracy_issues = sum(
            sum(len(v) for v in analysis.get("accuracy_analysis", {}).values() if isinstance(v, list))
            for analysis in file_analysis.values()
        )
        
        total_caching_opportunities = sum(
            len(analysis.get("caching_opportunities", []))
            for analysis in file_analysis.values()
        )
        
        self.results["performance_issues"] = {
            "total_memory_issues": total_memory_issues,
            "total_throughput_issues": total_throughput_issues,
            "total_accuracy_issues": total_accuracy_issues,
            "total_caching_opportunities": total_caching_opportunities
        }
        
        # Calcular score global
        if file_analysis:
            avg_performance_score = sum(
                analysis.get("overall_performance_score", 0) 
                for analysis in file_analysis.values()
            ) / len(file_analysis)
            
            self.results["overall_performance_score"] = avg_performance_score
        
        # Guardar an√°lisis detallado por archivo
        self.results["detailed_file_analysis"] = file_analysis
        
        print("\n‚úÖ AN√ÅLISIS COMPLETADO")
        print(f"üìä Score Performance Global: {self.results.get('overall_performance_score', 0):.1f}/10")
        print(f"üß† Total Memory Issues: {total_memory_issues}")
        print(f"‚ö° Total Throughput Issues: {total_throughput_issues}")  
        print(f"üéØ Total Accuracy Issues: {total_accuracy_issues}")
        print(f"üíæ Total Caching Opportunities: {total_caching_opportunities}")
        
        return self.results

def main():
    analyzer = PerformanceAnalyzer()
    
    # Archivos cr√≠ticos identificados como monol√≠ticos
    target_files = [
        "/workspace/supabase/functions/scraper-maxiconsumo/index.ts",
        "/workspace/supabase/functions/api-proveedor/index.ts"
    ]
    
    results = analyzer.run_analysis(target_files)
    
    # Guardar resultados
    with open("/workspace/docs/performance_analysis_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nüìã Resultados guardados en: /workspace/docs/performance_analysis_results.json")
    return results

if __name__ == "__main__":
    main()